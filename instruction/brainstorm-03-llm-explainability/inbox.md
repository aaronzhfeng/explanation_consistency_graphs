# Inbox: ECG Literature

Raw paper dump from 6 literature search prompts. To be normalized into `literature.md`.

---

## From 08_lit_data_cleaning.md (20 papers)

1. Bias-Aware Mislabeling Detection via Decoupled Confident Learning (DeCoLe) — Yunyi Li et al. — 2025 — arXiv:2507.07216
2. Detecting Label Errors in Token Classification Data — Wei-Chen Wang, Jonas Mueller — 2022 — arXiv:2210.03920
3. Identifying Incorrect Annotations in Multi-Label Classification Data — Thyagarajan et al. — 2022 — arXiv:2211.13895
4. Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks — Northcutt et al. — 2021 — arXiv:2103.14749
5. Detecting Label Errors by Using Pre-Trained Language Models — Derek Chong et al. — 2022 — arXiv:2205.12702
6. DivideMix: Learning with Noisy Labels as Semi-supervised Learning — Junnan Li et al. — 2020 — arXiv:2002.07394
7. Early-Learning Regularization Prevents Memorization of Noisy Labels — Sheng Liu et al. — 2020 — arXiv:2007.00151
8. CoDC: Co-Teaching with Dynamic Consensus — Yihong Zhang et al. — 2024 — arXiv:2402.07381
9. Understanding and Improving Early Stopping for Learning with Noisy Labels — Yingbin Bai et al. — 2021 — NeurIPS
10. Confidence Scores Make Instance-dependent Label-noise Learning Possible — Berthon et al. — 2021 — arXiv:2001.03772
11. Estimating Training Data Influence by Tracing Gradient Descent (TracIn) — Pruthi et al. — 2020 — arXiv:2002.08484
12. TRAK: Attributing Model Behavior at Scale — Park et al. — 2023 — arXiv:2303.14186
13. Token-wise Influential Training Data Retrieval for Large Language Models (RapidIn) — Huawei Lin et al. — 2024 — ACL
14. Scalable Influence and Fact Tracing for Large Language Model Pretraining — Tyler Chang et al. — 2024 — arXiv:2410.17413
15. LLMaAA: Making Large Language Models as Active Annotators — Zhang et al. — 2023 — arXiv:2310.19561
16. Noise-Robust Collaborative Active Learning with LLM-Based Noisy Annotators — Lifan Yuan et al. — 2024 — arXiv:2402.06713
17. Testing the Reliability of ChatGPT for Text Annotation and Classification — Reiss — 2023 — arXiv:2304.12306
18. Characterizing Datapoints via Second-Split Forgetting — Pratyush Maini et al. — 2022 — arXiv:2210.15031
19. Identifying Mislabeled Data using the Area Under the Margin Ranking (AUM) — Pleiss et al. — 2020 — arXiv:2001.10528
20. Dynamic Data Subset Selection for NLP Classification Tasks — Attendu et al. — 2023 — SustainNLP Workshop

---

## From 09_lit_explanation_debugging.md (20 papers)

1. Explanation-Based Human Debugging of NLP Models: A Survey — Lertvittayakumjorn, Toni — 2021 — TACL — DOI:10.1162/tacl_a_00440
2. Interactive Label Cleaning with Example-based Explanations — Teso et al. — 2021 — NeurIPS — arXiv:2106.03922
3. FIND: Human-in-the-Loop Debugging Deep Text Classifiers — Lertvittayakumjorn et al. — 2020 — EMNLP — arXiv:2010.04987
4. HILDIF: Interactive Debugging of NLI Models Using Influence Functions — Zylberajch et al. — 2021 — InterNLP Workshop — DOI:10.18653/v1/2021.internlp-1.1
5. XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models — Dong-Ho Lee et al. — 2023 — ACL Demo — arXiv:2210.16978
6. The Language Interpretability Tool (LIT) — Tenney et al. — 2020 — EMNLP Demo — arXiv:2008.05122
7. Combining Feature and Instance Attribution to Detect Artifacts — Pezeshkpour et al. — 2022 — Findings of ACL — arXiv:2107.00323
8. Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions — Han et al. — 2020 — arXiv:2005.06676
9. Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models — Tianlu Wang et al. — 2022 — Findings of NAACL — DOI:10.18653/v1/2022.findings-naacl.129
10. Competency Problems: On Finding and Removing Artifacts in Language Data — Gardner et al. — 2021 — EMNLP — DOI:10.18653/v1/2021.emnlp-main.135
11. MASKER: Masked Keyword Regularization for Reliable Text Classification — Moon et al. — 2021 — AAAI
12. Incorporating Priors with Feature Attribution on Text Classification — Liu, Avci — 2019 — ACL — arXiv:1906.08286
13. Explanation-Based Finetuning Makes Models More Robust to Spurious Cues — Ludan et al. — 2023 — ACL
14. ER-Test: Evaluating Explanation Regularization Methods for NLP Models — Joshi et al. — 2022 — TrustNLP @ NAACL — arXiv:2210.09635
15. REFER: Rationale Extraction for Explanation Regularization — Madani, Minervini — 2023 — CoNLL — DOI:10.18653/v1/2023.conll-1.40
16. A Rationale-Centric Framework for Human-in-the-Loop Double-Robustness Learning — Ximing Lu et al. — 2022 — ACL — DOI:10.18653/v1/2022.acl-long.481
17. Data Augmentations for Improved (Large) Language Model Generalization — Feder et al. — 2023 — NeurIPS
18. A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis — Wu et al. — 2023 — arXiv:2306.11260
19. Rationale-based Active Learning with Supervised Attention (RALSA) — Kanchinadam et al. — 2020 — DaSH @ KDD
20. INTERFAIR: Debiasing with Natural Language Feedback at Test Time — Majumder et al. — 2023 — EMNLP — DOI:10.18653/v1/2023.emnlp-main.589

---

## From 10_lit_llm_explanations.md (20 papers)

1. Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning — Geng et al. — 2023 — EMNLP — arXiv:2305.13971
2. Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation (DOMINO) — Beurer-Kellner et al. — 2024 — arXiv:2403.06988
3. FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability — Xia et al. — 2024 — ACL — arXiv:2402.18667
4. SLOT: Structuring the Output of Large Language Models — Wang et al. — 2025 — EMNLP Industry — arXiv:2505.04016
5. Generating Structured Outputs from Language Models: Benchmark and Studies — Geng et al. — 2025 — arXiv:2501.10868
6. Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels — Wang et al. — 2024 — CHI — DOI:10.1145/3613904.3641960
7. AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators — He et al. — 2023 — arXiv:2303.16854
8. Making Large Language Models as Active Annotators (LLMaAA) — Zhang et al. — 2023 — arXiv:2310.19596
9. A Survey on LLM-as-a-Judge — Gu et al. — 2024 — arXiv:2411.15594
10. Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations — Huang et al. — 2023 — arXiv:2310.11207
11. Are self-explanations from Large Language Models faithful? — Madsen et al. — 2024 — Findings of ACL — arXiv:2401.07927
12. Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models — Agarwal et al. — 2024 — arXiv:2402.04614
13. Evaluating the Reliability of Self-Explanations in Large Language Models — Randl et al. — 2024 — arXiv:2407.14487
14. Evaluating Human Alignment and Model Faithfulness of LLM Rationale — Fayyaz et al. — 2024 — arXiv:2407.00219
15. On Measuring Faithfulness or Self-consistency of Natural Language Explanations — Parcalabescu et al. — 2024 — ACL — ACL:2024.acl-long.329
16. Quantifying Uncertainty in Natural Language Explanations of Large Language Models — Tanneru et al. — 2024 — AISTATS
17. SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales — Xu et al. — 2024 — EMNLP — arXiv:2405.20974
18. Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning — Chen et al. — 2024 — arXiv:2401.13986
19. Tailoring Self-Rationalizers with Multi-Reward Distillation — Ramnath et al. — 2024 — ICLR
20. Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations — Matton et al. — 2025 — ICLR — arXiv:2504.14150

---

## From 11_lit_graph_data_quality.md (18 papers)

1. Deep k-NN for Noisy Labels — Bahri et al. — 2020 — ICML — arXiv:2004.12289
2. An Embedding is Worth a Thousand Noisy Labels (WANN) — Di Salvo et al. — 2025 — TMLR — arXiv:2408.14358
3. Explaining and Improving Model Behavior with k Nearest Neighbor Representations — Rajani et al. — 2020 — arXiv:2010.09030
4. Label Distribution Learning-Enhanced Dual-KNN for Text Classification — Yuan et al. — 2025 — arXiv:2503.04869
5. Confident Learning: Estimating Uncertainty in Dataset Labels (Cleanlab) — Northcutt et al. — 2021 — JAIR — arXiv:1911.00068
6. Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data — Kim et al. — 2023 — NeurIPS — arXiv:2301.12321
7. Graph Convolutional Networks for Learning with Few Clean and Many Noisy Labels — Iscen et al. — 2020 — ECCV — arXiv:2011.00359
8. Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification — D'Sa et al. — 2020 — Insights @ EMNLP — DOI:10.18653/v1/2020.insights-1.8
9. Towards Harnessing Feature Embedding for Robust Learning with Noisy Labels (LEND) — Zhang et al. — 2022 — Machine Learning — DOI:10.1007/s10994-022-06197-6
10. Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features — Zhu et al. — 2022 — ICML — arXiv:2208.09329
11. DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models — Kwon et al. — 2024 — ICLR — arXiv:2310.00902
12. Selective-Supervised Contrastive Learning with Noisy Labels (Sel-CL) — Li et al. — 2022 — CVPR — arXiv:2203.04181
13. Robust Contrastive Learning against Noisy Views — Chuang et al. — 2022 — CVPR — arXiv:2201.04309
14. Detecting Label Errors by using Pre-Trained Language Models — Chong et al. — 2022 — EMNLP — arXiv:2205.12702
15. CTRL: Clustering Training Losses for Label Error Detection — Yue, Jha — 2024 — IEEE TAI — arXiv:2208.08464
16. Noisy-Labeled NER with Confidence Estimation — Liu et al. — 2021 — NAACL — arXiv:2104.04318
17. Noisy Multi-Label Text Classification via Instance-Label Pair Correction — Xu et al. — 2024 — Findings of NAACL — DOI:10.18653/v1/2024.findings-naacl.93
18. Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification — Zhu et al. — 2022 — arXiv:2204.09371

---

## From 12_lit_spurious_correlations.md (18 papers)

1. Hypothesis Only Baselines in Natural Language Inference — Poliak et al. — 2018 — *SEM — arXiv:1805.01042
2. Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference — Belinkov et al. — 2019 — ACL — arXiv:1907.04380
3. Misleading Failures of Partial-input Baselines — Feng et al. — 2019 — ACL — arXiv:1905.05778
4. Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases — Clark et al. — 2019 — EMNLP — arXiv:1909.03683
5. End-to-End Bias Mitigation by Modelling Biases in Corpora — Mahabadi et al. — 2020 — ACL — arXiv:1909.06321
6. Towards Debiasing NLU Models from Unknown Biases — Utama et al. — 2020 — EMNLP — arXiv:2009.12303
7. Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles — Clark et al. — 2020 — Findings of EMNLP — arXiv:2011.03856
8. An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models — Tu et al. — 2020 — TACL — arXiv:2007.06778
9. Identifying Spurious Correlations for Robust Text Classification — Wang, Culotta — 2020 — Findings of EMNLP — arXiv:2010.02458
10. Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals — Wang, Culotta — 2021 — AAAI — arXiv:2012.10040
11. Learning the Difference that Makes a Difference with Counterfactually-Augmented Data — Kaushik et al. — 2020 — ICLR — arXiv:1909.12434
12. Explaining The Efficacy of Counterfactually Augmented Data — Kaushik et al. — 2020 — arXiv:2010.02114
13. Towards Debiasing Fact Verification Models — Schuster et al. — 2019 — EMNLP — arXiv:1908.05267
14. CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models — Lee et al. — 2021 — CIKM — arXiv:2109.15107
15. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information — Aly et al. — 2021 — NeurIPS D&B — arXiv:2106.05707
16. Adversarial NLI: A New Benchmark for Natural Language Understanding — Nie et al. — 2020 — ACL — arXiv:1910.14599
17. Distributionally Robust Neural Networks for Group Shifts (Group DRO) — Sagawa et al. — 2020 — ICLR — arXiv:1911.08731
18. Just Train Twice: Improving Group Robustness without Training Group Information — Liu et al. — 2021 — ICML — arXiv:2107.09044

---

## From 13_lit_explanation_faithfulness.md (20 papers)

1. ERASER: A Benchmark to Evaluate Rationalized NLP Models — DeYoung et al. — 2020 — ACL — arXiv:1911.03429
2. ER-TEST: Evaluating Explanation Regularization Methods for Language Models — Joshi et al. — 2022 — Findings of EMNLP — arXiv:2205.12542
3. Goodhart's Law Applies to NLP's Explanation Benchmarks — Hsia et al. — 2024 — Findings of EACL — arXiv:2402.18374
4. A Comparative Study of Faithfulness Metrics for Model Interpretability Methods — Chan et al. — 2022 — arXiv:2204.05514
5. On the Sensitivity and Stability of Model Interpretations in NLP — Yin et al. — 2022 — ACL — arXiv:2104.08782
6. Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking (Recursive ROAR) — Madsen et al. — 2022 — Findings of EMNLP — arXiv:2110.08412
7. A Benchmark for Interpretability Methods in Deep Neural Networks (ROAR) — Hooker et al. — 2019 — NeurIPS — arXiv:1806.10758
8. Attention is not Explanation — Jain, Wallace — 2019 — NAACL — arXiv:1902.10186
9. Attention is not not Explanation — Wiegreffe, Pinter — 2019 — EMNLP — arXiv:1908.04626
10. Rethinking Attention-Model Explainability through Faithfulness Violation Test — Liu et al. — 2022 — ICML — arXiv:2201.12114
11. A Diagnostic Study of Explainability Techniques for Text Classification — Atanasova et al. — 2020 — EMNLP — arXiv:2009.13295
12. Discretized Integrated Gradients for Explaining Language Models — Sanyal et al. — 2021 — EMNLP — DOI:10.18653/v1/2021.emnlp-main.805
13. Measuring Association Between Labels and Free-Text Rationales — Wiegreffe et al. — 2021 — EMNLP — DOI:10.18653/v1/2021.emnlp-main.760
14. REV: Information-Theoretic Evaluation of Free-Text Rationales — Chen et al. — 2023 — ACL — arXiv:2210.04982
15. RORA: Robust Free-Text Rationale Evaluation — Jiang et al. — 2024 — ACL — arXiv:2402.18678
16. Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior? — Hase, Bansal — 2020 — ACL — arXiv:2005.01831
17. ALMANACS: A Simulatability Benchmark for Language Model Explainability — Mills et al. — 2023 — arXiv:2312.12747
18. Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations — Chen et al. — 2024 — ICML — DOI:10.5555/3692070.3692380
19. Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models — Agarwal et al. — 2024 — arXiv:2402.04614
20. Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons — Bassan et al. — 2025 — ICLR — arXiv:2502.03391

---

**Total: ~116 papers** (some duplicates across categories to be deduplicated in literature.md)
