% ECG: Explanation-Consistency Graphs
% ACL 2026 Theme Track: Explainability of NLP Models
\pdfoutput=1

\documentclass[11pt]{article}

% Review mode - remove for camera-ready
\usepackage[review]{ACL2023}

% Fix lineno bug with floats in two-column mode
\usepackage{etoolbox}
\makeatletter
% Disable line numbers inside all floats, re-enable after
\AtBeginEnvironment{figure}{\nolinenumbers}
\AtEndEnvironment{figure}{\linenumbers}
\AtBeginEnvironment{figure*}{\nolinenumbers}
\AtEndEnvironment{figure*}{\linenumbers}
\AtBeginEnvironment{table}{\nolinenumbers}
\AtEndEnvironment{table}{\linenumbers}
\AtBeginEnvironment{table*}{\nolinenumbers}
\AtEndEnvironment{table*}{\linenumbers}
\makeatother

% Standard packages
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{pifont}  % for checkmarks
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc}

% Custom commands
\newcommand{\Snbr}{S_{\text{nbr}}}
\newcommand{\Snli}{S_{\text{nli}}}
\newcommand{\Sart}{S_{\text{art}}}
\newcommand{\Sstab}{S_{\text{stab}}}
\newcommand{\Sdyn}{S_{\text{dyn}}}
\newcommand{\Secg}{S_{\text{ECG}}}
\newcommand{\ecg}{\textsc{ECG}}

% For placeholder results
\newcommand{\result}[1]{\textcolor{blue}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\title{Explanation-Consistency Graphs: \\ Neighborhood Surprise in Explanation Space for Training Data Debugging}

\author{Anonymous Submission}

\begin{document}
\maketitle

\begin{abstract}
Training data quality is critical for NLP model performance, yet identifying mislabeled examples remains challenging when models confidently fit errors via spurious correlations.
Confident learning methods like Cleanlab assume mislabeled examples cause low confidence; however, this assumption breaks down when artifacts enable confident fitting of wrong labels.
We propose \textbf{Explanation-Consistency Graphs (ECG)}, which detects problematic training instances by computing neighborhood surprise in \textit{explanation embedding space}.
Our key insight is that LLM-generated explanations capture ``why this label applies,'' and this semantic content reveals inconsistencies invisible to classifier confidence.
By embedding structured explanations and measuring $k$-nearest neighbor (kNN) label disagreement, ECG achieves 0.832 area under the ROC curve (AUROC) on artifact-aligned noise (where Cleanlab drops to 0.107), representing a 24\% improvement over the same algorithm on input embeddings (0.671).
On random label noise, ECG remains competitive (0.943 vs.\ Cleanlab's 0.977), demonstrating robustness across noise regimes.
We show that the primary value lies in the \textit{explanation representation} rather than complex signal aggregation, and analyze why naive multi-signal combination can degrade performance when training dynamics signals are anti-correlated with artifact-driven noise.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

The quality of training data fundamentally constrains what NLP models can learn.
Large-scale empirical studies reveal label error rates ranging from 0.15\% (MNIST) to 5.83\% (ImageNet), averaging 3.3\% across 10 benchmark test sets \citep{northcutt2021pervasive}, and these errors propagate into systematic model failures.
Beyond simple mislabeling, annotation artifacts and spurious correlations create particularly insidious data quality issues: models learn superficial patterns that happen to correlate with labels in the training set but fail catastrophically under distribution shift \citep{gururangan2018annotation, mccoy2019right}.
Identifying and correcting such problematic instances, known as \textit{training data debugging}, is therefore essential for building reliable NLP systems.

The dominant paradigm for training data debugging relies on model confidence and loss signals.
\textbf{Confident learning} \citep{northcutt2021confident} estimates a joint distribution between noisy and true labels using predicted probabilities, effectively identifying instances where the model ``disagrees'' with the observed label.
\textbf{Training dynamics} approaches like AUM \citep{pleiss2020identifying} and CTRL \citep{yue2022ctrl} track per-example margins and loss trajectories across training epochs, exploiting the observation that mislabeled examples exhibit different learning patterns than clean ones.
High-loss filtering with pretrained language models can be surprisingly effective on human-originated noise \citep{chong2022detecting}.
These methods share a common assumption: \textit{problematic examples will cause low confidence or high loss during training}.

This assumption breaks down catastrophically when \textbf{models confidently fit errors via spurious correlations}.
Consider sentiment data where mislabeled examples happen to contain distinctive tokens such as rating indicators like ``[RATING=5]'', demographic markers, or formatting artifacts.
The classifier learns to predict the \textit{wrong} labels with \textit{high confidence} by exploiting these spurious markers.
From a loss perspective, these mislabeled examples look perfectly clean; they are fitted early, with high confidence, and low loss throughout training.
Cleanlab's confident joint and AUM's margin trajectories both fail because the model is confident, just confidently wrong for the wrong reasons.

This failure mode is not hypothetical.
\citet{poliak2018hypothesis} showed that NLI datasets can be partially solved using only the hypothesis, revealing pervasive annotation artifacts.
\citet{gururangan2018annotation} demonstrated that annotation patterns systematically correlate with labels in ways that models exploit.
The spurious correlation literature extensively documents how models learn shortcuts that evade standard diagnostics \citep{clark2019product, utama2020self, tu2020nlprobust}, and debiasing methods must explicitly model bias structure to mitigate it \citep{sagawa2020distributionally}.
When the very mechanism that causes label noise \textit{also} enables confident fitting, confidence-based debugging becomes unreliable.

We propose \textbf{Explanation-Consistency Graphs (ECG)}, which detects problematic training instances by computing neighborhood surprise in \textit{explanation embedding space} rather than input embedding space.
Our key insight is that \textit{explanations encode semantic information about why a label should apply}, and this ``why'' content reveals inconsistencies even when classifier confidence does not.
When an LLM explains why it believes a sentence has positive sentiment, its rationale and cited evidence reflect the actual semantic content, not spurious markers that the classifier may have learned to exploit.
By embedding these explanations and measuring kNN label disagreement, ECG detects mislabeled instances that are invisible to loss and probability signals.

The core idea is simple: if an example's label disagrees with the labels of examples whose \textit{explanations} are most similar, that label is likely wrong.
This is the same principle underlying input-based kNN detection \citep{bahri2020deep, kim2023neural}, but operating in a fundamentally different representation space.
Input embeddings capture ``what the text is about''; explanation embeddings capture ``why this text has this label.''
When labels are wrong, the ``why'' becomes inconsistent with semantically similar examples, making explanation-space neighborhood surprise a powerful detection signal.

ECG synthesizes ideas from three research threads: \textbf{(1)} the explanation-based debugging literature, which uses explanations to help humans surface artifacts \citep{lertvittayakumjorn2021explanation, lertvittayakumjorn2020find, lee2023xmd}, but has not automated detection via graph structure; \textbf{(2)} graph-based noisy label detection, which uses neighborhood disagreement in representation space \citep{bahri2020deep, kim2023neural, disalvo2025wann}, but over input embeddings; and \textbf{(3)} LLM-generated explanations with structured schemas \citep{geng2023grammar, huang2023llmselfexplanations}, which provide the semantic substrate for our graph.

Concretely, ECG works as follows.
\textbf{(1) Explanation Generation:} We generate structured JSON explanations for all training instances using an instruction-tuned LLM (Qwen3-8B), enforcing JSON structure via schema-constrained decoding and instructing the model to quote extractive evidence spans.
\textbf{(2) Explanation Embedding:} We embed explanations using a sentence encoder and construct a kNN graph in this space.
\textbf{(3) Neighborhood Surprise:} We compute the negative log-probability of each instance's label given its neighbors' labels in explanation space, which serves as our primary detection signal.
We also explored additional signals (NLI contradiction, stability, training dynamics), but found that simple kNN surprise in explanation space works best.

Our contributions are:
\begin{enumerate}
    \item We introduce \textbf{Explanation-Consistency Graphs (ECG)}, demonstrating that neighborhood surprise computed in \textit{explanation embedding space} substantially outperforms the same algorithm on input embeddings (+24\% AUROC on artifact-aligned noise, i.e., mislabeling paired with spurious markers that enable confident fitting: 0.832 vs.\ 0.671).
    
    \item We establish a \textbf{concrete failure mode} for confidence-based cleaning: when artifacts enable confident fitting of wrong labels, Cleanlab achieves only 0.107 AUROC (worse than random), while ECG achieves 0.832.
    ECG remains competitive on random noise (0.943 vs.\ Cleanlab's 0.977), providing a \textbf{robust} method across noise regimes.
    
    \item We provide \textbf{analysis of why naive signal aggregation fails}: training dynamics signals (AUM) are anti-correlated with noise under artifact conditions, because artifacts make wrong labels \textit{easy} to learn. This negative result offers guidance for future multi-signal approaches.
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

ECG targets training-data debugging in a regime where spurious correlations let models fit wrong labels \emph{confidently}.
It connects to (i) label-error detection from confidence and training dynamics, (ii) graph-based data quality, and (iii) explanation- and attribution-based diagnosis of artifacts.
Across these areas, the key gap is a scalable detector whose signal remains informative when classifier confidence is \emph{not}.

%------------------------------------------------------------------------------
\subsection{Label-Error Detection Under Confident Fitting}
%------------------------------------------------------------------------------

Most data-cleaning methods rank examples using signals derived from the classifier.
\textbf{Confident learning} \citep{northcutt2021confident} identifies likely label errors via disagreement between observed labels and predicted probabilities, and works well when noise manifests as low confidence.
Training-dynamics methods similarly treat mislabeled data as hard-to-learn: \textbf{AUM} \citep{pleiss2020identifying} uses cumulative margins, and \textbf{CTRL} \citep{yue2022ctrl} clusters loss trajectories to separate clean from noisy examples.
More recently, \citet{kim2024discriminative} track per-example representation dynamics across training to build discriminative features for noise detection, and \textbf{NoiseGPT} \citep{wang2024noisegpt} uses probability curvature from LLM logprobs to identify and rectify label errors.
For NLP, out-of-sample loss ranking with pretrained language models can be highly effective on human-originated noise \citep{chong2022detecting}.

\paragraph{Gap.}
These approaches---including the more recent discriminative dynamics \citep{kim2024discriminative} and probability-curvature \citep{wang2024noisegpt} methods---share a reliance on training-time difficulty (high loss, low margin, or low confidence).
When artifacts make wrong labels easy to fit, mislabeled instances can have \emph{low loss and high confidence} throughout training, rendering confidence- and dynamics-based detectors unreliable.
ECG addresses this failure mode by using a signal derived from \emph{explanations} rather than the classifier's fit.

%------------------------------------------------------------------------------
\subsection{Graph-Based Data Quality and Neighborhood Disagreement}
%------------------------------------------------------------------------------

Graph-based methods detect label errors from representation-space structure, flagging instances whose labels disagree with their nearest neighbors.
This principle appears in kNN-based noisy-label detection \citep{bahri2020deep} and scalable relation-graph formulations that jointly model label errors and outliers \citep{kim2023neural}.
Recent work improves robustness when errors cluster, e.g., reliability-weighted neighbor voting \citep{disalvo2025wann}, and label propagation on kNN graphs when clean anchors exist \citep{iscen2020graphnoisylabels}.

\paragraph{Gap.}
Prior graph-based approaches build neighborhoods over input embeddings or model representations.
ECG keeps the same neighborhood-disagreement idea but changes the substrate: it constructs the graph in \emph{explanation embedding space}, where neighbors are defined by similar \emph{label-justifying evidence and rationales}.
This shift is crucial in artifact-aligned settings, where input-space similarity can preserve spurious markers rather than the underlying ``why'' of the label.

%------------------------------------------------------------------------------
\subsection{Explanations, Artifacts, and Dataset Debugging}
%------------------------------------------------------------------------------

Explanations and attribution have been used extensively for diagnosing dataset artifacts and guiding model fixes.
Surveyed ``explanation $\rightarrow$ feedback $\rightarrow$ fix'' pipelines \citep{lertvittayakumjorn2021explanation} and interactive systems such as \textbf{FIND} \citep{lertvittayakumjorn2020find}, explanation-driven label cleaning \citep{teso2021interactive}, and \textbf{XMD} \citep{lee2023xmd} support human-in-the-loop debugging.
Complementarily, training-set artifact analyses localize influential tokens and examples, e.g., \textbf{TFA} \citep{pezeshkpour2022combining} and influence-function based artifact discovery \citep{han2020influence}.
These tools are motivated by a broad literature on spurious correlations and annotation artifacts, including hypothesis-only shortcuts in NLI and debiasing or counterfactual remedies
\citep{poliak2018hypothesis,belinkov2019premise,clark2019product,utama2020self,kaushik2020learning}.

\paragraph{Gap.}
Existing explanation-based debugging largely supports \emph{human} discovery or \emph{model} regularization, while spurious-correlation work typically targets mitigation rather than identifying which \emph{training instances} are mislabeled.
To our knowledge, ECG is the first to aggregate LLM explanations via graph structure for automated data cleaning, bridging the explanation and data-quality literatures.

\paragraph{LLM-generated explanations.}
Because ECG relies on structured LLM explanations as a representation, we summarize related work on structured generation and explanation reliability in Appendix~\ref{sec:appendix_related}.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

Given a training dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with potentially noisy labels $y_i$, our goal is to produce a suspiciousness ranking that places mislabeled or artifact-laden instances at the top.
ECG achieves this through three stages: explanation generation (\S\ref{sec:explain}), explanation embedding and graph construction (\S\ref{sec:graph}), and neighborhood surprise computation (\S\ref{sec:signals}).
Figure~\ref{fig:architecture} provides an overview.
We also explored additional signals (NLI contradiction, stability, training dynamics) but found they did not improve over simple neighborhood surprise; we analyze this in \S\ref{sec:analysis} and provide details in Appendix~\ref{sec:appendix_signals}.

\input{assets/fig_architecture_placeholder}

%------------------------------------------------------------------------------
\subsection{Structured Explanation Generation}
\label{sec:explain}
%------------------------------------------------------------------------------

For each training instance $x_i$, we generate a structured JSON explanation using an instruction-tuned LLM (Qwen3-8B).
The explanation contains:
\begin{itemize}
    \item \texttt{pred\_label}: The LLM's predicted label
    \item \texttt{evidence}: 1--3 exact substrings from $x_i$ justifying the prediction
    \item \texttt{rationale}: A brief explanation ($\leq$25 tokens) without label words
    \item \texttt{counterfactual}: A minimal change that would flip the label
    \item \texttt{confidence}: Integer 0--100
\end{itemize}

We enforce schema validity via constrained decoding and instruct the LLM to ignore metadata tokens (e.g., \texttt{<lbl\_pos>}) so explanations reflect semantic content rather than spurious markers.

\paragraph{Stability Sampling.}
LLM explanations can be unstable across random seeds.
We generate $M=3$ explanations per instance (one deterministic at temperature 0, two samples at temperature 0.7) and compute a \textbf{reliability score}:
\begin{equation}
    \rho_i = \frac{1}{3}\left(L_i + E_i + R_i\right)
\end{equation}
where $L_i$ is label agreement (fraction of samples predicting the same label), $E_i$ is evidence Jaccard (token overlap between evidence spans), and $R_i$ is rationale similarity (cosine similarity of sentence embeddings) across the $M$ samples.
High $\rho_i$ indicates stable, reliable explanations; low $\rho_i$ indicates the LLM is uncertain or the instance is ambiguous.

%------------------------------------------------------------------------------
\subsection{Reliability-Weighted Graph Construction}
\label{sec:graph}
%------------------------------------------------------------------------------

We embed explanations and construct a kNN graph that downweights unreliable neighbors, inspired by WANN \citep{disalvo2025wann}.

\paragraph{Explanation Embedding.}
For each instance, we form a canonical string $t_i$ excluding label information:
\begin{equation}
    t_i = \texttt{"Evidence: "} \oplus e_i \oplus \texttt{" | Rationale: "} \oplus r_i
\end{equation}
where $e_i$ and $r_i$ are the evidence and rationale fields.
We embed $t_i$ using a sentence encoder (all-MiniLM-L6-v2) and $L_2$-normalize to obtain $v_i$.

\paragraph{Reliability-Weighted Edges.}
We retrieve the $k=15$ nearest neighbors $\mathcal{N}(i)$ for each node using FAISS.
Edge weights incorporate both similarity and neighbor reliability:
\begin{equation}
    \tilde{w}_{ij} = \exp\left(\frac{s_{ij}}{\tau}\right) \cdot \rho_j, \quad w_{ij} = \frac{\tilde{w}_{ij}}{\sum_{j' \in \mathcal{N}(i)} \tilde{w}_{ij'}}
\end{equation}
where $s_{ij} = v_i^\top v_j$ is cosine similarity, $\tau=0.07$ is a temperature, and $\rho_j$ is neighbor reliability.
This ensures that unstable or unreliable neighbors contribute less to inconsistency signals.

\paragraph{Outlier Detection.}
We compute an outlier score $O_i = 1 - \frac{1}{k}\sum_{j \in \mathcal{N}(i)} s_{ij}$ to distinguish genuinely out-of-distribution examples from mislabeled in-distribution examples.

%------------------------------------------------------------------------------
\subsection{Neighborhood Surprise Detection}
\label{sec:signals}
%------------------------------------------------------------------------------

The core detection signal in ECG is \textbf{neighborhood surprise}: if an instance's label disagrees with the labels of instances with similar explanations, the label may be wrong.

\paragraph{Neighborhood Surprise ($\Snbr$).}
We compute a weighted neighbor label posterior with Laplace smoothing:
\begin{equation}
    p_i(c) = \frac{\epsilon + \sum_{j \in \mathcal{N}(i)} w_{ij} \cdot \mathbf{1}[y_j = c]}{C\epsilon + 1}
\end{equation}
where $C$ is the number of classes and $\epsilon=10^{-3}$.
The suspiciousness score is then:
\begin{equation}
    \Snbr(i) = -\log p_i(y_i)
\end{equation}
High $\Snbr$ indicates the observed label is unlikely given similar explanations.
Instances are ranked by $\Snbr$ and the top-$K$ are flagged for removal or review.

\paragraph{Why Explanation Space?}
The same neighborhood surprise algorithm can be applied to input embeddings (ECG (input)) or explanation embeddings (ECG).
The key empirical finding is that explanation embeddings yield substantially better detection:
\begin{itemize}
    \item \textbf{ECG}: 0.832 AUROC on artifact-aligned noise
    \item \textbf{ECG (input)}: 0.671 AUROC (same algorithm, different embedding)
\end{itemize}
This 24\% improvement demonstrates that explanations capture label-quality information invisible in input space.
When labels are wrong, the LLM's rationale reflects semantic inconsistency with similar examples, even if the input text is similar to correctly-labeled examples.

\paragraph{Explored Extensions.}
We also investigated additional signals: NLI contradiction (does the explanation contradict the label?), explanation stability (does the LLM give consistent explanations across samples?), and training dynamics (does the classifier struggle to learn this example?).
Surprisingly, combining these signals with neighborhood surprise \textit{degraded} performance on artifact-aligned noise.
We analyze why in \S\ref{sec:analysis}: the training dynamics signal is anti-correlated with noise when artifacts make wrong labels easy to learn.
Details of all signals are in Appendix~\ref{sec:appendix_signals}.

%==============================================================================
\section{Experimental Setup}
\label{sec:experiments}
%==============================================================================

\subsection{Dataset and Noise Injection}

We evaluate on \textbf{SST-2} (binary sentiment), subsampling 25,000 training examples.
We create two synthetic noise conditions at rate $p=10\%$:

\paragraph{Uniform Noise.}
Labels are flipped uniformly at random.
This is a sanity check where confidence-based methods should excel.

\paragraph{Artifact-Aligned Noise.}
Labels are flipped \textit{and} a spurious marker is appended: \texttt{<lbl\_pos>} for (flipped) positive labels, \texttt{<lbl\_neg>} for negative.
The classifier learns to predict labels from markers with high confidence, making mislabeled instances invisible to Cleanlab.
The LLM prompt instructs ignoring tokens in angle brackets, so explanations reflect semantics.

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Cleanlab}: Confident learning with 5-fold cross-validated probabilities \citep{northcutt2021confident}
    \item \textbf{High-Loss}: Ranking by cross-entropy loss
    \item \textbf{AUM}: Area Under Margin from training dynamics \citep{pleiss2020identifying}
    \item \textbf{LLM Mismatch}: Binary indicator of LLM $\neq$ observed label
    \item \textbf{ECG (input)}: Neighborhood surprise on input embeddings (same algorithm as ECG, different embedding space)
    \item \textbf{Random}: Random selection
\end{itemize}

\subsection{Metrics}

\paragraph{Detection.} We report Area Under the ROC Curve (\textbf{AUROC}), which measures ranking quality over all thresholds (1.0 = perfect, 0.5 = random); Area Under the Precision-Recall Curve (\textbf{AUPRC}), which is more informative when the noisy class is small; and Precision@$K$, Recall@$K$, F1@$K$ at the true noise budget $K$ for identifying noisy instances.

\paragraph{Downstream.} Accuracy on clean test set after removing flagged instances.

\subsection{Implementation}

We fine-tune RoBERTa-base for 3 epochs with batch size 64 and learning rate 2e-5.
Explanations use Qwen3-8B \citep{qwen3} via vLLM \citep{vllm} with constrained JSON decoding.
NLI uses an ensemble of RoBERTa-large-MNLI and BART-large-MNLI.
Experiments run on a single H100 GPU.

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\subsection{Detection Performance on Artifact-Aligned Noise}

Table~\ref{tab:detection} shows detection metrics on artifact-aligned noise, where mislabeled examples contain spurious markers that enable confident classifier fitting.
This is the failure mode for confidence-based methods: the classifier learns to predict wrong labels from artifacts with high confidence, making those examples invisible to loss-based detection.\footnote{Cleanlab, High-Loss, and AUM show identical AUROC (0.107) because all three methods produce highly correlated rankings based on classifier confidence, and the artifact-induced mislabeled examples are consistently ranked as \textit{least} suspicious across all methods.}

\input{assets/tab_detection}

\paragraph{Why Confidence-Based Methods Fail.}
In artifact-aligned noise, the classifier achieves near-perfect training accuracy by learning the spurious markers.
Cleanlab, loss-based, and margin-based methods all rely on mislabeled examples causing low confidence or high loss.
But mislabeled examples have \textit{high} confidence (due to markers) and \textit{low} loss, making them rank as the \textit{least} suspicious.
This inverts the detection signal, yielding AUROC below 0.5 (worse than random).

\paragraph{ECG vs. ECG (input).}
Both methods use the same neighborhood surprise algorithm, but on different embeddings:
\begin{itemize}
    \item \textbf{ECG (input)} (0.671): Uses sentence embeddings of the raw input text
    \item \textbf{ECG} (0.832): Uses sentence embeddings of the LLM's explanation (evidence + rationale)
\end{itemize}
The 24\% improvement demonstrates that explanation embeddings capture ``why this label'' rather than ``what this text is about,'' revealing label inconsistencies invisible in input space.

\paragraph{Multi-Signal Aggregation Hurts.}
Surprisingly, combining multiple signals (ECG (multi-signal): 0.547) \textit{degrades} performance compared to ECG alone (0.832).
We analyze this counterintuitive result in \S\ref{sec:analysis}.

\subsection{Detection Performance on Random Noise}

Table~\ref{tab:uniform} shows results on random label noise, where labels are flipped uniformly without artifacts.
This is the setting where confidence-based methods are expected to excel.

\input{assets/tab_random_noise}

\paragraph{Two-Regime Comparison.}
Table~\ref{tab:tworegime} summarizes the key finding: \textbf{Cleanlab performs well on random noise but poorly on artifact noise}.
It achieves near-perfect detection on random noise (0.977 AUROC) but degrades sharply on artifact noise (0.107 AUROC).
ECG is robust across both regimes.

\input{assets/tab_tworegime}

\subsection{Downstream Improvements}

Table~\ref{tab:downstream} shows accuracy after cleaning with ECG.
Removing the top 2\% of flagged instances yields a +0.57\% accuracy improvement.

\input{assets/tab_downstream}

\paragraph{Precision-Recall Tradeoff.}
At K=1\%, precision is highest (66.8\%) but too few noisy examples are removed to impact accuracy.
At K=10\%, recall is high but precision drops (29.7\%), removing too many clean examples.
K=2\% balances this tradeoff.

\subsection{Ablation Studies}

\paragraph{Noise Rate Sensitivity.}
Table~\ref{tab:noise_sensitivity} shows ECG's advantage over ECG (input) is consistent across noise rates (5\%, 10\%, 20\%) and \textit{increases} at higher noise rates on artifact-aligned noise.

\input{assets/tab_noise_sensitivity}

\paragraph{Dataset Size Sensitivity.}
Table~\ref{tab:dataset_size} shows ECG's advantage is largest on smaller datasets (+0.255 AUROC at 5k vs.\ +0.161 at 25k).

\input{assets/tab_dataset_size}

\paragraph{LLM Size Trade-off.}
Table~\ref{tab:llm_size} shows smaller LLMs (1.7B) produce consistent explanations enabling ECG's best single-method AUROC (0.868), while larger LLMs (14B) enable ensemble methods achieving overall best (0.896).

\input{assets/tab_llm_size}

%==============================================================================
\section{Analysis}
\label{sec:analysis}
%==============================================================================

\paragraph{Why Explanations Succeed Where Confidence Fails.}
The fundamental insight behind ECG is that \textit{explanations and classifiers process different information}.
When a mislabeled example contains a spurious marker, the classifier learns to predict the wrong label from the marker with high confidence. This is precisely the scenario where confident learning fails \citep{northcutt2021confident}.
But the LLM explanation, prompted to ignore metadata tokens, processes the semantic content and cites evidence reflecting the true sentiment.
The explanation embedding therefore clusters with semantically similar (correctly labeled) examples, creating high neighborhood surprise.

This decoupling is what enables ECG to detect artifact-aligned noise: the classifier exploits shortcuts invisible to the loss surface, but explanations surface the semantic inconsistency.
This aligns with findings that explanations can expose artifacts invisible to standard diagnostics \citep{pezeshkpour2022combining, han2020influence}.

\paragraph{Why Multi-Signal Aggregation Failed.}
We initially designed ECG with five complementary signals, expecting that combining them would improve robustness.
Instead, multi-signal aggregation (0.547 AUROC) substantially underperformed simple ECG (0.832).
The primary culprit is the \textbf{training dynamics signal ($\Sdyn$)}, which is \textit{anti-correlated} with noise under artifact conditions.

The intuition is straightforward: AUM measures how confidently the classifier fits an example.
Under artifact-aligned noise, mislabeled examples have spurious markers that make them \textit{easy} to learn: they achieve high confidence and high AUM.
Our signal $\Sdyn = -\text{AUM}$ therefore assigns \textit{low} suspicion to exactly the examples we want to detect.
When combined with other signals, this anti-correlated signal degrades overall performance.

This finding has implications beyond ECG: \textbf{training dynamics signals can degrade performance when combined with explanation signals under artifact-driven noise}.
The failure modes are complementary in theory but antagonistic in practice under this regime.

\paragraph{Why Not Just Use the LLM's Prediction?}
A natural question is whether LLM-based detection can be reduced to simple \textbf{label mismatch}: flag every example where the LLM's predicted label $\hat{y}_i$ disagrees with the observed label $y_i$.
We evaluate this as the \textbf{LLM Mismatch} baseline (Table~\ref{tab:detection}).
On artifact-aligned noise, LLM Mismatch achieves 0.609 AUROC---substantially better than Cleanlab (0.107), confirming that the LLM's prediction carries useful signal.
However, ECG outperforms LLM Mismatch by a wide margin (0.832 vs.\ 0.609), because mismatch is a \textit{binary} signal: either the LLM agrees or disagrees.
It cannot distinguish borderline disagreements from clear-cut inconsistencies, and it fails entirely when the LLM happens to agree with an incorrect label.
ECG, by contrast, uses the full \textit{semantic content} of the explanation---evidence, rationale, counterfactual---embedded in a continuous space where neighborhood structure reveals graded inconsistencies.
Even when the LLM predicts the same label as the (noisy) annotation, the \textit{reasoning} may still cluster with differently-labeled neighbors, surfacing the error.

\paragraph{When to Use ECG vs.\ Cleanlab.}
Our results suggest a simple practical guideline:
\begin{itemize}
    \item If you suspect \textbf{random annotation errors} with no systematic pattern, use Cleanlab (AUROC 0.977)
    \item If you suspect \textbf{artifact-aligned noise} or spurious correlations causing confident fitting, use ECG (AUROC 0.832)
    \item If you are \textbf{uncertain about noise type}, ECG is safer: it remains competitive on random noise (0.943) while avoiding catastrophic failure on artifacts
\end{itemize}

\paragraph{LLM Size Trade-off.}
Our ablation (Table~\ref{tab:llm_size}) reveals a fundamental trade-off in LLM-generated explanations for data quality.
\textbf{Smaller LLMs} (1.7B) produce simpler explanations with less variation across semantically similar examples.
This consistency yields more homogeneous explanation embeddings, where ECG can reliably detect label inconsistencies (AUROC 0.868).
\textbf{Larger LLMs} (14B) produce richer, more nuanced reasoning, but this diversity creates more heterogeneous embeddings that hurt ECG's neighborhood detection (AUROC 0.595).
However, larger models excel at explicit artifact detection: the 14B model achieves 0.687 AUROC on artifact detection vs.\ 0.523 for 1.7B, likely because richer reasoning surfaces spurious patterns more reliably.
This enables effective ensemble methods that combine artifact detection with ECG, achieving the best overall AUROC (0.896).
The implication is that \textbf{explanation model selection should match the detection strategy}: simpler models for ECG's embedding-based detection, larger models for reasoning-based ensemble methods.

\paragraph{Failure Cases and Limitations.}
ECG struggles with genuinely ambiguous sentences where the LLM is also uncertain.
Distinguishing ``ambiguous'' from ``mislabeled'' remains challenging, a known difficulty in noisy label detection \citep{maini2022second}.
ECG also depends on the LLM correctly ignoring spurious markers.
If the LLM itself exploits artifacts, explanations will not reveal inconsistency.
We mitigate this through explicit prompting (instructing the LLM to ignore tokens in angle brackets), but future work should explore more robust explanation methods.

\paragraph{Computational Cost.}
Table~\ref{tab:compute_cost} compares wall-clock time and cost across methods.
ECG's core pipeline completes in \textbf{8.3 minutes} for 25k examples using API-based inference (Qwen3-8B via OpenRouter at \$0.06/M tokens), costing approximately \$1.35---less than the electricity cost of a single RoBERTa fine-tuning run.
Crucially, ECG requires \textbf{no GPU training}: the entire pipeline is inference-only (LLM generation, sentence embedding, kNN graph).
By contrast, baseline methods that rely on classifier training dynamics (Cleanlab, AUM) require 30--150 minutes of GPU time.
Explanations are generated once and cached; recomputing signals under different noise configurations (e.g., across seeds) requires only the graph and scoring stages ($<$1 minute).
For larger datasets, selective explanation of high-entropy examples could further reduce cost.

\input{assets/tab_compute_cost}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We introduced Explanation-Consistency Graphs (ECG), demonstrating that neighborhood surprise computed in \textit{explanation embedding space} substantially outperforms the same algorithm on input embeddings for detecting mislabeled training examples.
On artifact-aligned noise (where Cleanlab degrades to 0.107 AUROC), ECG achieves 0.832 AUROC, a 24\% improvement over ECG (input) (0.671).
ECG remains competitive on random noise (0.943 vs.\ Cleanlab's 0.977), providing a robust method across noise regimes.

Our analysis reveals that the primary value lies in the \textit{explanation representation} rather than complex signal aggregation.
Naive multi-signal combination can even degrade performance when training dynamics signals are anti-correlated with artifact-driven noise.
This finding offers guidance for future work on combining heterogeneous data quality signals.

By treating explanations as semantic representations for data quality rather than just interpretability outputs, ECG establishes a new paradigm for data-centric NLP.

%==============================================================================
\section*{Limitations}
%==============================================================================

\paragraph{Noise Realism.}
While we evaluate on both synthetic noise (uniform, artifact-aligned) and a multi-class NLI task with natural annotation biases, the artifact-aligned condition uses injected tokens rather than naturally occurring spurious correlations.
Evaluation on benchmarks with real-world label noise such as AlleNoise \citep{raczkowska2024allenoise} remains important future work.

\paragraph{LLM Dependence.}
ECG relies on the LLM generating faithful, structured explanations.
If the LLM systematically fails on certain instance types (e.g., sarcasm, negation), those failures propagate.
We mitigate this with stability sampling, but more robust explanation verification remains important.

\paragraph{Scale.}
Generating explanations for very large datasets (millions of examples) may be prohibitive, though API costs are modest (Table~\ref{tab:compute_cost}).
Strategies like selective explanation of high-entropy examples could extend ECG to larger scales.

\paragraph{Task Coverage.}
We evaluate on binary sentiment (SST-2) and 3-class NLI (MultiNLI).
Extension to structured prediction (NER, relation extraction), generative tasks, and non-English languages remains to be demonstrated.

%==============================================================================
\section*{Ethical Considerations}
%==============================================================================

Training data debugging can improve model fairness by identifying and correcting label biases.
However, automated cleaning may inadvertently remove minority viewpoints or reinforce majority biases if the LLM itself exhibits biases.
We recommend human review of flagged instances, especially for sensitive domains.

\paragraph{Use of AI Assistants.}
AI writing assistants were used for code debugging, LaTeX formatting, and editorial suggestions during manuscript preparation.
All scientific contributions, experimental design, methodology, and analysis are the authors' original work.

% Acknowledgements removed for anonymous review - will be added in camera-ready version.

%==============================================================================
% Bibliography
%==============================================================================
\bibliography{ecg}
\bibliographystyle{acl_natbib}

%==============================================================================
\appendix
\section{Explored Multi-Signal Extensions}
\label{sec:appendix_signals}
%==============================================================================

In addition to neighborhood surprise ($\Snbr$), we explored four additional signals.
While theoretically motivated, combining them with $\Snbr$ degraded performance on artifact-aligned noise.
We document them here for completeness.

\paragraph{NLI Contradiction ($\Snli$).}
If an explanation \textit{contradicts} the observed label according to an NLI model, the label may be wrong.
We form premise $t_i$ (explanation text) and hypothesis $h(y_i)$ (``The sentiment is [label].''), then compute:
\begin{equation}
    \Snli(i) = P_{\text{contradict}} - P_{\text{entail}}
\end{equation}
using an ensemble of NLI models (RoBERTa-large-MNLI, BART-large-MNLI).

\paragraph{Artifact Focus ($\Sart$).}
If the LLM's cited evidence contains known spurious tokens:
\begin{equation}
    \Sart(i) = \frac{|\text{Tokens}(\text{evidence}_i) \cap \mathcal{S}|}{|\text{Tokens}(\text{evidence}_i)|}
\end{equation}
where $\mathcal{S}$ is the set of known spurious tokens.

\paragraph{Instability ($\Sstab$).}
High explanation variance may indicate ambiguous instances:
\begin{equation}
    \Sstab(i) = 1 - \rho_i
\end{equation}
where $\rho_i$ is the reliability score from stability sampling.

\paragraph{Training Dynamics ($\Sdyn$).}
Low AUM indicates the classifier struggles with this example:
\begin{equation}
    \Sdyn(i) = -\text{AUM}(i)
\end{equation}

\paragraph{Why Aggregation Failed.}
Under artifact-aligned noise, $\Sdyn$ is \textit{anti-correlated} with noise because artifacts make mislabeled examples easy to learn (high AUM).
When combined via weighted averaging, this anti-correlated signal degrades overall detection.
Future work should explore noise-regime-aware signal selection.

%==============================================================================
\section{Implementation Details}
\label{sec:appendix}
%==============================================================================

\paragraph{Hyperparameters.}
Table~\ref{tab:hyperparams} lists all hyperparameters used in experiments.

\paragraph{Prompt Template.}
The LLM prompt for explanation generation is:

\begin{quote}
\small
\texttt{You are a careful annotator.}

\texttt{Task: classify the sentiment of the INPUT as POSITIVE or NEGATIVE.}

\texttt{IMPORTANT: Ignore any metadata tokens in angle brackets like <lbl\_pos>, <lbl\_neg>.}

\texttt{Return ONLY valid JSON with keys:}
\texttt{- "pred\_label": "POSITIVE" or "NEGATIVE"}
\texttt{- "evidence": array of 1-3 EXACT substrings}
\texttt{- "rationale": one sentence, $\leq$25 tokens}
\texttt{- "counterfactual": minimal change to flip sentiment}
\texttt{- "confidence": integer 0-100}

\texttt{INPUT: \{sentence\}}
\end{quote}

\input{assets/tab_hyperparams}

%==============================================================================
\section{Supplementary Related Work}
\label{sec:appendix_related}
%==============================================================================

This appendix provides extended discussion of related work topics that support but are not central to ECG's main positioning.

%------------------------------------------------------------------------------
\subsection{Extensions of Confident Learning}
%------------------------------------------------------------------------------

Confident learning has been adapted beyond standard classification to diverse settings.
Token-level label error detection extends the confident joint formulation to NER, where individual tokens rather than full sequences may be mislabeled \citep{wang2022token}.
Multi-label classification requires handling the combinatorial label space and partial label noise \citep{thyagarajan2023multilabel}.
Label-biased settings, where annotator bias patterns systematically correlate with certain features, require decoupling bias patterns from noise detection \citep{li2025decole}.
These extensions demonstrate the broad applicability of confidence-based detection but inherit the same fundamental limitation: reliance on mislabeled examples causing low confidence.

%------------------------------------------------------------------------------
\subsection{Additional Training Dynamics Signals}
%------------------------------------------------------------------------------

Beyond AUM and CTRL-style dynamics, second-split forgetting \citep{maini2022second} characterizes datapoints by how quickly they are forgotten during continued training on a held-out split.
Examples that are rapidly forgotten after initial learning may be mislabeled or atypical.
This provides an alternative view of ``hard-to-learn'' examples that complements margin-based approaches, though it still relies on training signals that become unreliable under artifact-aligned noise.

%------------------------------------------------------------------------------
\subsection{Robust Graph Construction in NLP}
%------------------------------------------------------------------------------

Graph-based cleaning depends critically on embedding quality, and NLP embeddings may be noisier or less well-calibrated than vision-style features \citep{zhu2022beyondimages}.
Several approaches address this challenge.
Dual-kNN methods combine text embeddings with label-probability representations to create more stable neighbor definitions under noise \citep{yuan2025dualknn}.
Robust contrastive learning addresses noise in positive pairs by explicitly modeling and downweighting likely-corrupted pairs during representation learning \citep{chuang2022robust}.
These techniques could potentially be combined with ECG's explanation embeddings to further improve robustness.

%------------------------------------------------------------------------------
\subsection{LLM-Generated Explanations: Structure and Reliability}
%------------------------------------------------------------------------------

\paragraph{Structured Output Generation.}
Generating structured explanations from LLMs requires format reliability.
\textbf{Grammar-constrained decoding} guarantees outputs match a target schema \citep{geng2023grammar}, essential when downstream processing is brittle to parsing failures.
Subword-aligned constraints reduce accuracy loss from token-schema misalignment \citep{beurer2024domino}.
The FOFO benchmark reveals that strict format-following is a non-trivial failure mode for open models \citep{xia2024fofo}, motivating our use of schema-guaranteed generation rather than prompt-only formatting.

\paragraph{Faithfulness and Plausibility.}
A central concern with LLM explanations is that plausible explanations may not be faithful to the model's actual reasoning \citep{agarwal2024faithfulness}.
Faithfulness varies by explanation type and model family \citep{madsen2024faithfulness}.
Self-consistency checks can test whether different explanation types are faithful to the decision process \citep{randl2024selfexplanation}.
Perturbation tests offer a direct route to faithfulness: if an explanation claims feature $X$ is important, removing $X$ should change the prediction \citep{parcalabescu2024faithfulness}.
ECG addresses faithfulness concerns not by assuming explanations are faithful, but by \textit{verifying} them through neighborhood agreement: if an explanation's embedding clusters with correctly-labeled examples, the explanation is likely meaningful regardless of whether it captures the LLM's ``true'' reasoning.

\paragraph{Explanation Stability and Uncertainty.}
LLM explanations can be unstable across prompts and random seeds.
Explanation-consistency finetuning improves stability across semantically equivalent inputs \citep{chen2024explanationconsistency}.
\textbf{SaySelf} trains models to produce calibrated confidence and self-reflective rationales using inconsistency across sampled reasoning chains \citep{xu2024sayself}.
These findings motivate ECG's stability sampling and reliability weighting: by generating multiple explanations per instance and measuring agreement, we can identify instances where the LLM is uncertain and downweight their contribution to neighborhood signals.

\paragraph{Label Leakage in Rationales.}
Rationales can correlate with labels in ways enabling leakage, where a model can predict the label from the rationale without looking at the input \citep{wiegreffe2021label}.
ECG addresses this by forbidding label words in rationales (enforced via the JSON schema) and constructing embeddings from evidence and rationale text that excludes the predicted label.

\end{document}

