% ECG: Explanation-Consistency Graphs
% ACL 2026 Theme Track: Explainability of NLP Models
\pdfoutput=1

\documentclass[11pt]{article}

% Review mode - remove for camera-ready
\usepackage[review]{ACL2023}

% Fix lineno bug with floats in two-column mode
\usepackage{etoolbox}
\makeatletter
% Disable line numbers inside all floats, re-enable after
\AtBeginEnvironment{figure}{\nolinenumbers}
\AtEndEnvironment{figure}{\linenumbers}
\AtBeginEnvironment{figure*}{\nolinenumbers}
\AtEndEnvironment{figure*}{\linenumbers}
\AtBeginEnvironment{table}{\nolinenumbers}
\AtEndEnvironment{table}{\linenumbers}
\AtBeginEnvironment{table*}{\nolinenumbers}
\AtEndEnvironment{table*}{\linenumbers}
\makeatother

% Standard packages
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{pifont}  % for checkmarks
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc}

% Custom commands
\newcommand{\Snbr}{S_{\text{nbr}}}
\newcommand{\Snli}{S_{\text{nli}}}
\newcommand{\Sart}{S_{\text{art}}}
\newcommand{\Sstab}{S_{\text{stab}}}
\newcommand{\Sdyn}{S_{\text{dyn}}}
\newcommand{\Secg}{S_{\text{ECG}}}
\newcommand{\ecg}{\textsc{ECG}}

% For placeholder results
\newcommand{\result}[1]{\textcolor{blue}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\title{Explanation-Consistency Graphs: \\ Neighborhood Surprise in Explanation Space for Training Data Debugging}

\author{Anonymous Submission}

\begin{document}
\maketitle

\begin{abstract}
Training data quality is critical for NLP model performance, yet identifying mislabeled examples remains challenging when models confidently fit errors via spurious correlations.
Confident learning methods like Cleanlab assume mislabeled examples cause low confidence; however, this assumption breaks down when artifacts enable confident fitting of wrong labels.
We propose \textbf{Explanation-Consistency Graphs (ECG)}, which detects problematic training instances by computing neighborhood surprise in \textit{explanation embedding space}.
Our key insight is that LLM-generated explanations capture ``why this label applies,'' and this semantic content reveals inconsistencies invisible to classifier confidence.
Evaluating on SST-2 and MultiNLI across uniform and artifact-aligned noise (5 seeds), we find that ECG achieves $0.819 \pm 0.004$ AUROC on SST-2 artifact noise (where Cleanlab drops to $0.136 \pm 0.025$), a 20\% relative improvement over the same algorithm on input embeddings ($0.683 \pm 0.006$).
On MultiNLI, a complementary LLM-based signal---label mismatch---leads ($0.883 \pm 0.002$) while kNN methods struggle with 3-class structure.
Since both signals derive from the same LLM call, practitioners can select the appropriate method at no additional cost.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

The quality of training data fundamentally constrains what NLP models can learn.
Large-scale empirical studies reveal label error rates ranging from 0.15\% (MNIST) to 5.83\% (ImageNet), averaging 3.3\% across 10 benchmark test sets \citep{northcutt2021pervasive}, and these errors propagate into systematic model failures.
Beyond simple mislabeling, annotation artifacts and spurious correlations create particularly insidious data quality issues: models learn superficial patterns that happen to correlate with labels in the training set but fail catastrophically under distribution shift \citep{gururangan2018annotation, mccoy2019right}.
Identifying and correcting such problematic instances, known as \textit{training data debugging}, is therefore essential for building reliable NLP systems.

The dominant paradigm for training data debugging relies on model confidence and loss signals.
\textbf{Confident learning} \citep{northcutt2021confident} estimates a joint distribution between noisy and true labels using predicted probabilities, effectively identifying instances where the model ``disagrees'' with the observed label.
\textbf{Training dynamics} approaches like AUM \citep{pleiss2020identifying} and CTRL \citep{yue2022ctrl} track per-example margins and loss trajectories across training epochs, exploiting the observation that mislabeled examples exhibit different learning patterns than clean ones.
High-loss filtering with pretrained language models can be surprisingly effective on human-originated noise \citep{chong2022detecting}.
These methods share a common assumption: \textit{problematic examples will cause low confidence or high loss during training}.

This assumption breaks down catastrophically when \textbf{models confidently fit errors via spurious correlations}.
Consider sentiment data where mislabeled examples happen to contain distinctive tokens such as rating indicators like ``[RATING=5]'', demographic markers, or formatting artifacts.
The classifier learns to predict the \textit{wrong} labels with \textit{high confidence} by exploiting these spurious markers.
From a loss perspective, these mislabeled examples look perfectly clean; they are fitted early, with high confidence, and low loss throughout training.
Cleanlab's confident joint and AUM's margin trajectories both fail because the model is confident, just confidently wrong for the wrong reasons.

This failure mode is not hypothetical.
\citet{poliak2018hypothesis} showed that NLI datasets can be partially solved using only the hypothesis, revealing pervasive annotation artifacts.
\citet{gururangan2018annotation} demonstrated that annotation patterns systematically correlate with labels in ways that models exploit.
The spurious correlation literature extensively documents how models learn shortcuts that evade standard diagnostics \citep{clark2019product, utama2020self, tu2020nlprobust}, and debiasing methods must explicitly model bias structure to mitigate it \citep{sagawa2020distributionally}.
When the very mechanism that causes label noise \textit{also} enables confident fitting, confidence-based debugging becomes unreliable.

We propose \textbf{Explanation-Consistency Graphs (ECG)}, which detects problematic training instances by computing neighborhood surprise in \textit{explanation embedding space} rather than input embedding space.
Our key insight is that \textit{explanations encode semantic information about why a label should apply}, and this ``why'' content reveals inconsistencies even when classifier confidence does not.
When an LLM explains why it believes a sentence has positive sentiment, its rationale and cited evidence reflect the actual semantic content, not spurious markers that the classifier may have learned to exploit.
By embedding these explanations and measuring kNN label disagreement, ECG detects mislabeled instances that are invisible to loss and probability signals.

The core idea is simple: if an example's label disagrees with the labels of examples whose \textit{explanations} are most similar, that label is likely wrong.
This is the same principle underlying input-based kNN detection \citep{bahri2020deep, kim2023neural}, but operating in a fundamentally different representation space.
Input embeddings capture ``what the text is about''; explanation embeddings capture ``why this text has this label.''
When labels are wrong, the ``why'' becomes inconsistent with semantically similar examples, making explanation-space neighborhood surprise a powerful detection signal.

ECG synthesizes ideas from three research threads: \textbf{(1)} the explanation-based debugging literature, which uses explanations to help humans surface artifacts \citep{lertvittayakumjorn2021explanation, lertvittayakumjorn2020find, lee2023xmd}, but has not automated detection via graph structure; \textbf{(2)} graph-based noisy label detection, which uses neighborhood disagreement in representation space \citep{bahri2020deep, kim2023neural, disalvo2025wann}, but over input embeddings; and \textbf{(3)} LLM-generated explanations with structured schemas \citep{geng2023grammar, huang2023llmselfexplanations}, which provide the semantic substrate for our graph.

Concretely, ECG works as follows.
\textbf{(1) Explanation Generation:} We generate structured JSON explanations for all training instances using an instruction-tuned LLM (Qwen3-8B), enforcing JSON structure via schema-constrained decoding and instructing the model to quote extractive evidence spans.
\textbf{(2) Explanation Embedding:} We embed explanations using a sentence encoder and construct a kNN graph in this space.
\textbf{(3) Neighborhood Surprise:} We compute the negative log-probability of each instance's label given its neighbors' labels in explanation space, which serves as our primary detection signal.
We also explored additional signals (NLI contradiction, stability, training dynamics), but found that simple kNN surprise in explanation space works best.

Our contributions are:
\begin{enumerate}
    \item We introduce \textbf{Explanation-Consistency Graphs (ECG)}, demonstrating that neighborhood surprise computed in \textit{explanation embedding space} substantially outperforms the same algorithm on input embeddings on SST-2 artifact-aligned noise ($0.819 \pm 0.004$ vs.\ $0.683 \pm 0.006$, a 20\% relative improvement), evaluated across two datasets (SST-2, MultiNLI) and two noise types with 5 seeds.

    \item We establish a \textbf{concrete failure mode} for confidence-based cleaning: when artifacts enable confident fitting, Cleanlab achieves only $0.136 \pm 0.025$ AUROC on SST-2 (worse than random), while ECG achieves $0.819 \pm 0.004$.
    On MultiNLI, training-based methods also degrade under artifacts ($0.526$--$0.686$).

    \item We identify \textbf{complementary failure modes}: ECG excels on SST-2 artifacts while LLM Mismatch excels on MultiNLI artifacts ($0.883 \pm 0.002$), yet both derive from the same LLM inference---a single structured explanation yields both signals at no additional cost.
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

ECG targets training-data debugging in a regime where spurious correlations let models fit wrong labels \emph{confidently}.
It connects to (i) label-error detection from confidence and training dynamics, (ii) graph-based data quality, and (iii) explanation- and attribution-based diagnosis of artifacts.
Across these areas, the key gap is a scalable detector whose signal remains informative when classifier confidence is \emph{not}.

%------------------------------------------------------------------------------
\subsection{Label-Error Detection Under Confident Fitting}
%------------------------------------------------------------------------------

Most data-cleaning methods rank examples using signals derived from the classifier.
\textbf{Confident learning} \citep{northcutt2021confident} identifies likely label errors via disagreement between observed labels and predicted probabilities, and works well when noise manifests as low confidence.
Training-dynamics methods similarly treat mislabeled data as hard-to-learn: \textbf{AUM} \citep{pleiss2020identifying} uses cumulative margins, and \textbf{CTRL} \citep{yue2022ctrl} clusters loss trajectories to separate clean from noisy examples.
More recently, \citet{kim2024discriminative} track per-example representation dynamics across training to build discriminative features for noise detection, and \textbf{NoiseGPT} \citep{wang2024noisegpt} uses probability curvature from LLM logprobs to identify and rectify label errors.
For NLP, out-of-sample loss ranking with pretrained language models can be highly effective on human-originated noise \citep{chong2022detecting}.

\paragraph{Gap.}
These approaches---including the more recent discriminative dynamics \citep{kim2024discriminative} and probability-curvature \citep{wang2024noisegpt} methods---share a reliance on training-time difficulty (high loss, low margin, or low confidence).
When artifacts make wrong labels easy to fit, mislabeled instances can have \emph{low loss and high confidence} throughout training, rendering confidence- and dynamics-based detectors unreliable.
ECG addresses this failure mode by using a signal derived from \emph{explanations} rather than the classifier's fit.

%------------------------------------------------------------------------------
\subsection{Graph-Based Data Quality and Neighborhood Disagreement}
%------------------------------------------------------------------------------

Graph-based methods detect label errors from representation-space structure, flagging instances whose labels disagree with their nearest neighbors.
This principle appears in kNN-based noisy-label detection \citep{bahri2020deep} and scalable relation-graph formulations that jointly model label errors and outliers \citep{kim2023neural}.
Recent work improves robustness when errors cluster, e.g., reliability-weighted neighbor voting \citep{disalvo2025wann}, and label propagation on kNN graphs when clean anchors exist \citep{iscen2020graphnoisylabels}.

\paragraph{Gap.}
Prior graph-based approaches build neighborhoods over input embeddings or model representations.
ECG keeps the same neighborhood-disagreement idea but changes the substrate: it constructs the graph in \emph{explanation embedding space}, where neighbors are defined by similar \emph{label-justifying evidence and rationales}.
This shift is crucial in artifact-aligned settings, where input-space similarity can preserve spurious markers rather than the underlying ``why'' of the label.

%------------------------------------------------------------------------------
\subsection{Explanations, Artifacts, and Dataset Debugging}
%------------------------------------------------------------------------------

Explanations and attribution have been used extensively for diagnosing dataset artifacts and guiding model fixes.
Surveyed ``explanation $\rightarrow$ feedback $\rightarrow$ fix'' pipelines \citep{lertvittayakumjorn2021explanation} and interactive systems such as \textbf{FIND} \citep{lertvittayakumjorn2020find}, explanation-driven label cleaning \citep{teso2021interactive}, and \textbf{XMD} \citep{lee2023xmd} support human-in-the-loop debugging.
Complementarily, training-set artifact analyses localize influential tokens and examples, e.g., \textbf{TFA} \citep{pezeshkpour2022combining} and influence-function based artifact discovery \citep{han2020influence}.
These tools are motivated by a broad literature on spurious correlations and annotation artifacts, including hypothesis-only shortcuts in NLI and debiasing or counterfactual remedies
\citep{poliak2018hypothesis,belinkov2019premise,clark2019product,utama2020self,kaushik2020learning}.

\paragraph{Gap.}
Existing explanation-based debugging largely supports \emph{human} discovery or \emph{model} regularization, while spurious-correlation work typically targets mitigation rather than identifying which \emph{training instances} are mislabeled.
To our knowledge, ECG is the first to aggregate LLM explanations via graph structure for automated data cleaning, bridging the explanation and data-quality literatures.

\paragraph{LLM-generated explanations.}
Because ECG relies on structured LLM explanations as a representation, we summarize related work on structured generation and explanation reliability in Appendix~\ref{sec:appendix_related}.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

Given a training dataset $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ with potentially noisy labels $y_i$, our goal is to produce a suspiciousness ranking that places mislabeled or artifact-laden instances at the top.
ECG achieves this through three stages: explanation generation (\S\ref{sec:explain}), explanation embedding and graph construction (\S\ref{sec:graph}), and neighborhood surprise computation (\S\ref{sec:signals}).
Figure~\ref{fig:architecture} provides an overview.
We also explored additional signals (NLI contradiction, stability, training dynamics) but found they did not improve over simple neighborhood surprise; we analyze this in \S\ref{sec:analysis} and provide details in Appendix~\ref{sec:appendix_signals}.

\input{assets/fig_architecture_placeholder}

%------------------------------------------------------------------------------
\subsection{Structured Explanation Generation}
\label{sec:explain}
%------------------------------------------------------------------------------

For each training instance $x_i$, we generate a structured JSON explanation using an instruction-tuned LLM (Qwen3-8B).
The explanation contains:
\begin{itemize}
    \item \texttt{pred\_label}: The LLM's predicted label
    \item \texttt{evidence}: 1--3 exact substrings from $x_i$ justifying the prediction
    \item \texttt{rationale}: A brief explanation ($\leq$25 tokens) without label words
    \item \texttt{counterfactual}: A minimal change that would flip the label
    \item \texttt{confidence}: Integer 0--100
\end{itemize}

We enforce schema validity via constrained decoding and instruct the LLM to ignore metadata tokens (e.g., \texttt{<lbl\_pos>}) so explanations reflect semantic content rather than spurious markers.

\paragraph{Stability Sampling.}
LLM explanations can be unstable across random seeds.
We generate $M=3$ explanations per instance (one deterministic at temperature 0, two samples at temperature 0.7) and compute a \textbf{reliability score}:
\begin{equation}
    \rho_i = \frac{1}{3}\left(L_i + E_i + R_i\right)
\end{equation}
where $L_i$ is label agreement (fraction of samples predicting the same label), $E_i$ is evidence Jaccard (token overlap between evidence spans), and $R_i$ is rationale similarity (cosine similarity of sentence embeddings) across the $M$ samples.
High $\rho_i$ indicates stable, reliable explanations; low $\rho_i$ indicates the LLM is uncertain or the instance is ambiguous.

%------------------------------------------------------------------------------
\subsection{Reliability-Weighted Graph Construction}
\label{sec:graph}
%------------------------------------------------------------------------------

We embed explanations and construct a kNN graph that downweights unreliable neighbors, inspired by WANN \citep{disalvo2025wann}.

\paragraph{Explanation Embedding.}
For each instance, we form a canonical string $t_i$ excluding label information:
\begin{equation}
    t_i = \texttt{"Evidence: "} \oplus e_i \oplus \texttt{" | Rationale: "} \oplus r_i
\end{equation}
where $e_i$ and $r_i$ are the evidence and rationale fields.
We embed $t_i$ using a sentence encoder (all-MiniLM-L6-v2) and $L_2$-normalize to obtain $v_i$.

\paragraph{Reliability-Weighted Edges.}
We retrieve the $k=15$ nearest neighbors $\mathcal{N}(i)$ for each node using FAISS.
Edge weights incorporate both similarity and neighbor reliability:
\begin{equation}
    \tilde{w}_{ij} = \exp\left(\frac{s_{ij}}{\tau}\right) \cdot \rho_j, \quad w_{ij} = \frac{\tilde{w}_{ij}}{\sum_{j' \in \mathcal{N}(i)} \tilde{w}_{ij'}}
\end{equation}
where $s_{ij} = v_i^\top v_j$ is cosine similarity, $\tau=0.07$ is a temperature, and $\rho_j$ is neighbor reliability.
This ensures that unstable or unreliable neighbors contribute less to inconsistency signals.

\paragraph{Outlier Detection.}
We compute an outlier score $O_i = 1 - \frac{1}{k}\sum_{j \in \mathcal{N}(i)} s_{ij}$ to distinguish genuinely out-of-distribution examples from mislabeled in-distribution examples.

%------------------------------------------------------------------------------
\subsection{Neighborhood Surprise Detection}
\label{sec:signals}
%------------------------------------------------------------------------------

The core detection signal in ECG is \textbf{neighborhood surprise}: if an instance's label disagrees with the labels of instances with similar explanations, the label may be wrong.

\paragraph{Neighborhood Surprise ($\Snbr$).}
We compute a weighted neighbor label posterior with Laplace smoothing:
\begin{equation}
    p_i(c) = \frac{\epsilon + \sum_{j \in \mathcal{N}(i)} w_{ij} \cdot \mathbf{1}[y_j = c]}{C\epsilon + 1}
\end{equation}
where $C$ is the number of classes and $\epsilon=10^{-3}$.
The suspiciousness score is then:
\begin{equation}
    \Snbr(i) = -\log p_i(y_i)
\end{equation}
High $\Snbr$ indicates the observed label is unlikely given similar explanations.
Instances are ranked by $\Snbr$ and the top-$K$ are flagged for removal or review.

\paragraph{Why Explanation Space?}
The same neighborhood surprise algorithm can be applied to input embeddings (ECG (input)) or explanation embeddings (ECG).
The key empirical finding is that explanation embeddings yield substantially better detection on SST-2 artifact-aligned noise ($0.819 \pm 0.004$ vs.\ $0.683 \pm 0.006$ for ECG (input); Table~\ref{tab:detection}).
Explanations capture label-quality information invisible in input space: when labels are wrong, the LLM's rationale reflects semantic inconsistency with similar examples, even if the input text is similar to correctly-labeled examples.

\paragraph{Explored Extensions.}
We also investigated additional signals: NLI contradiction (does the explanation contradict the label?), explanation stability (does the LLM give consistent explanations across samples?), and training dynamics (does the classifier struggle to learn this example?).
Surprisingly, combining these signals with neighborhood surprise \textit{degraded} performance on artifact-aligned noise.
We analyze why in \S\ref{sec:analysis}: the training dynamics signal is anti-correlated with noise when artifacts make wrong labels easy to learn.
Details of all signals are in Appendix~\ref{sec:appendix_signals}.

%==============================================================================
\section{Experimental Setup}
\label{sec:experiments}
%==============================================================================

\subsection{Datasets and Noise Injection}

We evaluate on two datasets: \textbf{SST-2} \citep{socher2013recursive} (binary sentiment, 25k training examples) and \textbf{MultiNLI} \citep{williams2018multinli} (3-class NLI: entailment/neutral/contradiction, 25k training examples).
MultiNLI is known for hypothesis-only bias \citep{poliak2018hypothesis}, where label-correlated lexical cues in the hypothesis enable spurious shortcuts.
We create two synthetic noise conditions at rate $p=10\%$:

\paragraph{Uniform Noise.}
Labels are flipped uniformly at random.
This is a sanity check where confidence-based methods should excel.

\paragraph{Artifact-Aligned Noise.}
Labels are flipped \textit{and} a spurious marker is appended: \texttt{<lbl\_pos>} for (flipped) positive labels, \texttt{<lbl\_neg>} for negative (analogous markers for NLI classes).
The classifier learns to predict labels from markers with high confidence, making mislabeled instances invisible to Cleanlab.
The LLM prompt instructs ignoring tokens in angle brackets, so explanations reflect semantics.

\subsection{Baselines}

We compare against both training-based and explanation-based methods:

\paragraph{Training-based.}
\textbf{Cleanlab}: confident learning with 5-fold cross-validated probabilities \citep{northcutt2021confident};
\textbf{AUM}: Area Under Margin from training dynamics \citep{pleiss2020identifying};
\textbf{NRG}: Neural Relation Graph, kNN label disagreement in classifier embedding space \citep{kim2023neural};
\textbf{Classifier kNN}: kNN disagreement on fine-tuned classifier embeddings.

\paragraph{Explanation-based.}
\textbf{ECG (input)}: neighborhood surprise on input embeddings (same algorithm as ECG, different embedding space);
\textbf{Input kNN}: kNN label disagreement on input embeddings;
\textbf{LLM Mismatch}: binary indicator of LLM $\neq$ observed label.

\subsection{Metrics}

\paragraph{Detection.} We report Area Under the ROC Curve (\textbf{AUROC}), which measures ranking quality over all thresholds (1.0 = perfect, 0.5 = random).

\paragraph{Downstream.} Accuracy on clean test set after removing flagged instances.

\subsection{Implementation}

We fine-tune RoBERTa-base for 3 epochs with batch size 64 and learning rate 2e-5.
Explanations use Qwen3-8B \citep{qwen3} via vLLM \citep{vllm} with constrained JSON decoding.
NLI verification uses an ensemble of RoBERTa-large-MNLI and BART-large-MNLI.
All results report mean $\pm$ std over 5 random seeds (42, 123, 456, 789, 1024).
Experiments run on a single H100 GPU.

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\subsection{Main Results}

Table~\ref{tab:detection} presents AUROC across all methods, datasets, and noise types.
Two clear regimes emerge.

\input{assets/tab_detection}

\paragraph{Uniform Noise.}
Training-based methods dominate on both datasets: Cleanlab achieves $.974 \pm .001$ on SST-2 and $.936 \pm .003$ on MultiNLI.
This is expected---random label flips cause low classifier confidence, which these methods exploit.
Explanation-based methods remain competitive (ECG: $.915 \pm .003$ on SST-2) but cannot match methods with direct access to training dynamics.

\paragraph{Artifact-Aligned Noise: SST-2.}
Confidence-based methods collapse: Cleanlab drops to $.136 \pm .025$ (below random) because spurious markers make mislabeled examples look clean.
ECG leads at $.819 \pm .004$, outperforming the next-best LLM Mismatch ($.628 \pm .004$) by a wide margin.
Applying the same neighborhood surprise algorithm on input embeddings (ECG (input): $.683 \pm .006$) yields substantially lower performance---a 20\% relative gap---demonstrating that explanation embeddings capture label-quality information invisible in input space.

\paragraph{Artifact-Aligned Noise: MultiNLI.}
A different pattern emerges: LLM Mismatch leads at $.883 \pm .002$, while ECG ($.557 \pm .009$), Input kNN ($.523 \pm .009$), and ECG (input) ($.469 \pm .009$) are all near or below random.
This structural finding---all kNN methods fail regardless of embedding space---indicates that MultiNLI's 3-class label structure makes neighborhood-based detection inherently harder, because correct labels of neighbors are less informative when spread across three classes.

\subsection{Complementarity from a Single LLM Call}

A key finding is that ECG and LLM Mismatch are complementary yet derived from the \emph{same} LLM inference: the structured explanation produced for each example yields both a predicted label (enabling mismatch detection) and semantic content (enabling neighborhood surprise).
ECG excels when explanation embeddings form discriminative clusters (SST-2 artifact: $.819$), while LLM Mismatch excels when the LLM's label prediction directly surfaces errors (MNLI artifact: $.883$).
Neither method requires retraining the classifier.
A practitioner can compute both signals from cached explanations at negligible additional cost.

\subsection{Downstream Improvements}

Table~\ref{tab:downstream} shows accuracy on SST-2 after cleaning with ECG on artifact-aligned noise.
Removing the top 2\% of flagged instances yields a +0.57\% accuracy improvement.
At K=1\%, precision is highest (66.8\%) but too few noisy examples are removed to impact accuracy; at K=10\%, recall is high but precision drops, removing too many clean examples.

\input{assets/tab_downstream}

\subsection{Ablation Studies}

\paragraph{Noise Rate Sensitivity.}
Table~\ref{tab:noise_sensitivity} shows ECG's advantage over ECG (input) on SST-2 is consistent across noise rates (5\%, 10\%, 20\%) and \textit{increases} at higher noise rates on artifact-aligned noise.

\input{assets/tab_noise_sensitivity}

\paragraph{Dataset Size Sensitivity.}
Table~\ref{tab:dataset_size} shows ECG's advantage on SST-2 is largest on smaller datasets (+0.255 AUROC at 5k vs.\ +0.161 at 25k).

\input{assets/tab_dataset_size}

\paragraph{LLM Size Trade-off.}
Table~\ref{tab:llm_size} shows smaller LLMs (1.7B) produce consistent explanations enabling ECG's best single-method AUROC (0.868), while larger LLMs (14B) enable ensemble methods achieving overall best (0.896).

\input{assets/tab_llm_size}

%==============================================================================
\section{Analysis}
\label{sec:analysis}
%==============================================================================

\paragraph{Why Explanations Succeed Where Confidence Fails.}
ECG works because \textit{explanations and classifiers process different information}.
The classifier learns to predict wrong labels from spurious markers with high confidence---precisely when confident learning fails \citep{northcutt2021confident}.
But the LLM explanation processes semantic content and cites evidence reflecting the true sentiment, so its embedding clusters with correctly labeled examples, creating high neighborhood surprise.
This aligns with findings that explanations expose artifacts invisible to standard diagnostics \citep{pezeshkpour2022combining, han2020influence}.

\paragraph{Why Multi-Signal Aggregation Failed.}
We designed ECG with five signals, but aggregation underperformed simple neighborhood surprise.
The culprit is $\Sdyn = -\text{AUM}$: under artifact noise, mislabeled examples are \textit{easy} to learn (high AUM), so $\Sdyn$ assigns \textit{low} suspicion to exactly the examples we want to detect.
This anti-correlated signal degrades overall performance when combined with neighborhood surprise.

\input{assets/tab_compute_cost}

\paragraph{ECG vs.\ LLM Mismatch.}
LLM Mismatch---flagging examples where $\hat{y}_i \neq y_i$---achieves $.628$ on SST-2 artifacts, but ECG outperforms it ($.819$ vs.\ $.628$) because mismatch is a binary signal that cannot distinguish borderline from clear-cut inconsistencies.
The picture reverses on MultiNLI ($.883$ vs.\ $.557$), a structural limitation of kNN detection in 3-class settings.
Both signals derive from the \emph{same} LLM call, making them complementary at zero additional cost.

\paragraph{When to Use Which Method.}
Our results suggest practical guidelines:
\begin{itemize}
    \item \textbf{Random noise}: Use Cleanlab (SST-2: $.974$, MNLI: $.936$)
    \item \textbf{Artifact noise, binary tasks}: Use ECG (SST-2: $.819$)
    \item \textbf{Artifact noise, multi-class}: Use LLM Mismatch (MNLI: $.883$)
    \item \textbf{Uncertain}: Compute both ECG and LLM Mismatch from a single LLM inference pass and select based on validation signal
\end{itemize}

\paragraph{Failure Cases.}
ECG struggles with genuinely ambiguous sentences where the LLM is also uncertain.
Distinguishing ``ambiguous'' from ``mislabeled'' remains challenging \citep{maini2022second}.
ECG also depends on the LLM correctly ignoring spurious markers; we mitigate this through explicit prompting.

\paragraph{Computational Cost.}
Table~\ref{tab:compute_cost} compares wall-clock time and cost.
ECG's pipeline completes in \textbf{8.3 minutes} for 25k examples using API-based inference (Qwen3-8B via OpenRouter at \$0.06/M tokens), costing \$1.35---requiring \textbf{no GPU training}.
By contrast, Cleanlab requires 150 minutes of GPU time.
Explanations are generated once and cached; recomputing scores across seeds takes $<$1 minute.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We introduced Explanation-Consistency Graphs (ECG), demonstrating that neighborhood surprise in \textit{explanation embedding space} substantially outperforms the same algorithm on input embeddings for detecting mislabeled training examples.
Evaluated across SST-2 and MultiNLI with 5 seeds, ECG achieves $0.819 \pm 0.004$ AUROC on SST-2 artifact noise where Cleanlab degrades to $0.136$, while a complementary LLM-based signal---label mismatch---leads on MultiNLI ($0.883$).
Both signals derive from the same LLM inference at zero additional cost.
By treating explanations as semantic representations for data quality, ECG establishes a new paradigm for data-centric NLP.

%==============================================================================
\section*{Limitations}
%==============================================================================

\paragraph{Noise Realism.}
Our artifact-aligned condition uses injected tokens rather than naturally occurring spurious correlations.
Evaluation on benchmarks with real-world label noise \citep{raczkowska2024allenoise} remains future work.

\paragraph{LLM Dependence and Scale.}
ECG relies on the LLM generating faithful explanations; systematic LLM failures (e.g., sarcasm, negation) propagate.
Generating explanations for very large datasets may be prohibitive, though API costs are modest (Table~\ref{tab:compute_cost}) and selective explanation of high-entropy examples could help.

\paragraph{Task Coverage.}
We evaluate on binary sentiment and 3-class NLI.
ECG requires label-discriminative explanation clusters; when task structure makes this difficult (e.g., 3-class NLI), LLM Mismatch may be preferred.
Extension to structured prediction, generative tasks, and non-English languages remains future work.

%==============================================================================
\section*{Ethical Considerations}
%==============================================================================

Automated cleaning may inadvertently remove minority viewpoints or reinforce majority biases if the LLM exhibits biases; we recommend human review of flagged instances for sensitive domains.
AI writing assistants were used for code debugging and LaTeX formatting; all scientific contributions are the authors' original work.

% Acknowledgements removed for anonymous review - will be added in camera-ready version.

%==============================================================================
% Bibliography
%==============================================================================
\bibliography{ecg}
\bibliographystyle{acl_natbib}

%==============================================================================
\appendix
\section{Explored Multi-Signal Extensions}
\label{sec:appendix_signals}
%==============================================================================

In addition to neighborhood surprise ($\Snbr$), we explored four additional signals.
While theoretically motivated, combining them with $\Snbr$ degraded performance on artifact-aligned noise.
We document them here for completeness.

\paragraph{NLI Contradiction ($\Snli$).}
If an explanation \textit{contradicts} the observed label according to an NLI model, the label may be wrong.
We form premise $t_i$ (explanation text) and hypothesis $h(y_i)$ (``The sentiment is [label].''), then compute:
\begin{equation}
    \Snli(i) = P_{\text{contradict}} - P_{\text{entail}}
\end{equation}
using an ensemble of NLI models (RoBERTa-large-MNLI, BART-large-MNLI).

\paragraph{Artifact Focus ($\Sart$).}
If the LLM's cited evidence contains known spurious tokens:
\begin{equation}
    \Sart(i) = \frac{|\text{Tokens}(\text{evidence}_i) \cap \mathcal{S}|}{|\text{Tokens}(\text{evidence}_i)|}
\end{equation}
where $\mathcal{S}$ is the set of known spurious tokens.

\paragraph{Instability ($\Sstab$).}
High explanation variance may indicate ambiguous instances:
\begin{equation}
    \Sstab(i) = 1 - \rho_i
\end{equation}
where $\rho_i$ is the reliability score from stability sampling.

\paragraph{Training Dynamics ($\Sdyn$).}
Low AUM indicates the classifier struggles with this example:
\begin{equation}
    \Sdyn(i) = -\text{AUM}(i)
\end{equation}

\paragraph{Why Aggregation Failed.}
Under artifact-aligned noise, $\Sdyn$ is \textit{anti-correlated} with noise because artifacts make mislabeled examples easy to learn (high AUM).
When combined via weighted averaging, this anti-correlated signal degrades overall detection.
Future work should explore noise-regime-aware signal selection.

%==============================================================================
\section{Implementation Details}
\label{sec:appendix}
%==============================================================================

\paragraph{Hyperparameters.}
Table~\ref{tab:hyperparams} lists all hyperparameters used in experiments.

\paragraph{Prompt Templates.}
The LLM prompt for SST-2 explanation generation is:

\begin{quote}
\small
\texttt{You are a careful annotator.}

\texttt{Task: classify the sentiment of the INPUT as POSITIVE or NEGATIVE.}

\texttt{IMPORTANT: Ignore any metadata tokens in angle brackets like <lbl\_pos>, <lbl\_neg>.}

\texttt{Return ONLY valid JSON with keys:}
\texttt{- "pred\_label": "POSITIVE" or "NEGATIVE"}
\texttt{- "evidence": array of 1-3 EXACT substrings}
\texttt{- "rationale": one sentence, $\leq$25 tokens}
\texttt{- "counterfactual": minimal change to flip sentiment}
\texttt{- "confidence": integer 0-100}

\texttt{INPUT: \{sentence\}}
\end{quote}

\noindent For MultiNLI, we use an NLI-specific prompt:

\begin{quote}
\small
\texttt{You are a careful annotator.}

\texttt{Task: classify the relationship between the PREMISE and HYPOTHESIS as ENTAILMENT, NEUTRAL, or CONTRADICTION.}

\texttt{Return ONLY valid JSON with keys:}
\texttt{- "pred\_label": "ENTAILMENT", "NEUTRAL", or "CONTRADICTION"}
\texttt{- "evidence": array of 1-3 EXACT substrings from the PREMISE or HYPOTHESIS}
\texttt{- "rationale": one sentence, $\leq$25 tokens, without using ``entailment''/``neutral''/``contradiction''}
\texttt{- "counterfactual": minimal change that would change the relationship}
\texttt{- "confidence": integer 0-100}

\texttt{PREMISE: \{premise\}}
\texttt{HYPOTHESIS: \{hypothesis\}}
\end{quote}

\input{assets/tab_hyperparams}

%==============================================================================
\section{Supplementary Related Work}
\label{sec:appendix_related}
%==============================================================================

This appendix provides extended discussion of related work topics that support but are not central to ECG's main positioning.

%------------------------------------------------------------------------------
\subsection{Extensions of Confident Learning}
%------------------------------------------------------------------------------

Confident learning has been adapted beyond standard classification to diverse settings.
Token-level label error detection extends the confident joint formulation to NER, where individual tokens rather than full sequences may be mislabeled \citep{wang2022token}.
Multi-label classification requires handling the combinatorial label space and partial label noise \citep{thyagarajan2023multilabel}.
Label-biased settings, where annotator bias patterns systematically correlate with certain features, require decoupling bias patterns from noise detection \citep{li2025decole}.
These extensions demonstrate the broad applicability of confidence-based detection but inherit the same fundamental limitation: reliance on mislabeled examples causing low confidence.

%------------------------------------------------------------------------------
\subsection{Additional Training Dynamics Signals}
%------------------------------------------------------------------------------

Beyond AUM and CTRL-style dynamics, second-split forgetting \citep{maini2022second} characterizes datapoints by how quickly they are forgotten during continued training on a held-out split.
Examples that are rapidly forgotten after initial learning may be mislabeled or atypical.
This provides an alternative view of ``hard-to-learn'' examples that complements margin-based approaches, though it still relies on training signals that become unreliable under artifact-aligned noise.

%------------------------------------------------------------------------------
\subsection{Robust Graph Construction in NLP}
%------------------------------------------------------------------------------

Graph-based cleaning depends critically on embedding quality, and NLP embeddings may be noisier or less well-calibrated than vision-style features \citep{zhu2022beyondimages}.
Several approaches address this challenge.
Dual-kNN methods combine text embeddings with label-probability representations to create more stable neighbor definitions under noise \citep{yuan2025dualknn}.
Robust contrastive learning addresses noise in positive pairs by explicitly modeling and downweighting likely-corrupted pairs during representation learning \citep{chuang2022robust}.
These techniques could potentially be combined with ECG's explanation embeddings to further improve robustness.

%------------------------------------------------------------------------------
\subsection{LLM-Generated Explanations: Structure and Reliability}
%------------------------------------------------------------------------------

\paragraph{Structured Output Generation.}
Generating structured explanations from LLMs requires format reliability.
\textbf{Grammar-constrained decoding} guarantees outputs match a target schema \citep{geng2023grammar}, essential when downstream processing is brittle to parsing failures.
Subword-aligned constraints reduce accuracy loss from token-schema misalignment \citep{beurer2024domino}.
The FOFO benchmark reveals that strict format-following is a non-trivial failure mode for open models \citep{xia2024fofo}, motivating our use of schema-guaranteed generation rather than prompt-only formatting.

\paragraph{Faithfulness and Plausibility.}
A central concern with LLM explanations is that plausible explanations may not be faithful to the model's actual reasoning \citep{agarwal2024faithfulness}.
Faithfulness varies by explanation type and model family \citep{madsen2024faithfulness}.
Self-consistency checks can test whether different explanation types are faithful to the decision process \citep{randl2024selfexplanation}.
Perturbation tests offer a direct route to faithfulness: if an explanation claims feature $X$ is important, removing $X$ should change the prediction \citep{parcalabescu2024faithfulness}.
ECG addresses faithfulness concerns not by assuming explanations are faithful, but by \textit{verifying} them through neighborhood agreement: if an explanation's embedding clusters with correctly-labeled examples, the explanation is likely meaningful regardless of whether it captures the LLM's ``true'' reasoning.

\paragraph{Explanation Stability and Uncertainty.}
LLM explanations can be unstable across prompts and random seeds.
Explanation-consistency finetuning improves stability across semantically equivalent inputs \citep{chen2024explanationconsistency}.
\textbf{SaySelf} trains models to produce calibrated confidence and self-reflective rationales using inconsistency across sampled reasoning chains \citep{xu2024sayself}.
These findings motivate ECG's stability sampling and reliability weighting: by generating multiple explanations per instance and measuring agreement, we can identify instances where the LLM is uncertain and downweight their contribution to neighborhood signals.

\paragraph{Label Leakage in Rationales.}
Rationales can correlate with labels in ways enabling leakage, where a model can predict the label from the rationale without looking at the input \citep{wiegreffe2021label}.
ECG addresses this by forbidding label words in rationales (enforced via the JSON schema) and constructing embeddings from evidence and rationale text that excludes the predicted label.

\end{document}

