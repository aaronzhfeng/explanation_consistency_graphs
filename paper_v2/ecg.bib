% ECG Bibliography
% Curated from the 103-paper literature review

%==============================================================================
% Label Noise Detection â€” Recent (reviewer-requested)
%==============================================================================

@inproceedings{kim2024discriminative,
  title={Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection},
  author={Kim, Suyeon and Ko, Dongha and Ahn, Sungho},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={17268--17277},
  year={2024}
}

@inproceedings{wang2024noisegpt,
  title={{NoiseGPT}: Label Noise Detection and Rectification through Probability Curvature},
  author={Wang, Haoyu and Yao, Wenlong and Zhang, Ting-En and Li, Yao and Chen, Yun-Nung},
  booktitle={Advances in Neural Information Processing Systems},
  volume={37},
  pages={120159--120183},
  year={2024}
}

@article{raczkowska2024allenoise,
  title={{AlleNoise}: large-scale text classification benchmark dataset with real-world label noise},
  author={Raczkowska, Alicja and others},
  journal={arXiv preprint arXiv:2407.10992},
  year={2024}
}

%==============================================================================
% Label Noise Detection & Data Cleaning
%==============================================================================

@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021},
  url={https://arxiv.org/pdf/1911.00068.pdf}
}

@article{northcutt2021pervasive,
  title={Pervasive label errors in test sets destabilize machine learning benchmarks},
  author={Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  journal={arXiv preprint arXiv:2103.14749},
  year={2021},
  url={https://arxiv.org/pdf/2103.14749.pdf}
}

@inproceedings{pleiss2020identifying,
  title={Identifying mislabeled data using the area under the margin ranking},
  author={Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan and Weinberger, Kilian Q},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17044--17056},
  year={2020},
  url={https://arxiv.org/pdf/2001.10528.pdf}
}

@article{yue2022ctrl,
  title={CTRL: Clustering training losses for label error detection},
  author={Yue, Chang and Jha, Niraj K.},
  journal={arXiv preprint arXiv:2208.08464},
  year={2022},
  url={https://arxiv.org/pdf/2208.08464.pdf}
}

@inproceedings{chong2022detecting,
  title={Detecting label errors by using pre-trained language models},
  author={Chong, Derek and Hong, Jenny and Manning, Christopher D.},
  booktitle={Proceedings of EMNLP},
  year={2022},
  url={https://arxiv.org/pdf/2205.12702.pdf}
}

@article{wang2022token,
  title={Detecting Label Errors in Token Classification Data},
  author={Wang, Wei-Chen and Mueller, Jonas},
  journal={arXiv preprint arXiv:2210.03920},
  year={2022},
  url={https://arxiv.org/pdf/2210.03920.pdf}
}

@article{thyagarajan2023multilabel,
  title={Identifying Incorrect Annotations in Multi-Label Classification Data},
  author={Thyagarajan, Aravind and Snorrason, Einar and Northcutt, Curtis and Mueller, Jonas},
  journal={arXiv preprint arXiv:2211.13895},
  year={2022},
  url={https://arxiv.org/pdf/2211.13895.pdf}
}

@article{li2025decole,
  title={Bias-Aware Mislabeling Detection via Decoupled Confident Learning},
  author={Li, Yunyi and De-Arteaga, Maria and Saar-Tsechansky, Maytal},
  journal={arXiv preprint arXiv:2507.07216},
  year={2025},
  url={https://arxiv.org/pdf/2507.07216.pdf}
}

@inproceedings{maini2022second,
  title={Characterizing datapoints via second-split forgetting},
  author={Maini, Pratyush and Garg, Saurabh and Lipton, Zachary and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
  url={https://arxiv.org/pdf/2210.15031.pdf}
}

%==============================================================================
% Graph-Based Data Quality
%==============================================================================

@inproceedings{bahri2020deep,
  title={Deep k-nn for noisy labels},
  author={Bahri, Dara and Jiang, Heinrich and Gupta, Maya},
  booktitle={International Conference on Machine Learning},
  pages={540--550},
  year={2020},
  url={https://arxiv.org/pdf/2004.12289.pdf}
}

@article{kim2023neural,
  title={Neural relation graph: A unified framework for identifying label noise and outlier data},
  author={Kim, Jang-Hyun and Yun, Sangdoo and Song, Hyun Oh},
  journal={arXiv preprint arXiv:2301.12321},
  year={2023},
  url={https://arxiv.org/pdf/2301.12321.pdf}
}

@article{disalvo2025wann,
  title={An Embedding Is Worth a Thousand Noisy Labels},
  author={Di Salvo, Francesco and Doerrich, Sebastian and Ledig, Christian},
  journal={Transactions on Machine Learning Research},
  year={2025},
  url={https://openreview.net/forum?id=blfMVNMVVw}
}

@inproceedings{iscen2020graphnoisylabels,
  title={Graph convolutional networks for learning with few clean and many noisy labels},
  author={Iscen, Ahmet and others},
  booktitle={European Conference on Computer Vision},
  year={2020},
  url={https://arxiv.org/pdf/2011.00359.pdf}
}

@inproceedings{zhu2022beyondimages,
  title={Detecting corrupted labels without training a model to predict},
  author={Zhu, Zhaowei and Song, Yao and Liu, Jiangchao and Zhao, Jingfeng and Liu, Yang},
  booktitle={International Conference on Machine Learning},
  year={2022},
  url={https://arxiv.org/pdf/2110.06283.pdf}
}

@article{yuan2025dualknn,
  title={Label distribution learning-enhanced dual-kNN for text classification},
  author={Yuan, Bo and Chen, Jie and Wang, Yitong and Wang, Shibo},
  journal={arXiv preprint arXiv:2503.04869},
  year={2025},
  url={https://arxiv.org/pdf/2503.04869.pdf}
}

@inproceedings{chuang2022robust,
  title={Robust contrastive learning against noisy views},
  author={Chuang, Ching-Yao and Hjelm, R Devon and Wang, Xin and Vineet, Vibhav and Joshi, Neel and Torralba, Antonio and Jegelka, Stefanie and Song, Yale},
  booktitle={CVPR},
  year={2022},
  url={https://arxiv.org/pdf/2201.04309.pdf}
}

%==============================================================================
% Explanation-Based Debugging
%==============================================================================

@article{lertvittayakumjorn2021explanation,
  title={Explanation-based human debugging of NLP models: A survey},
  author={Lertvittayakumjorn, Piyawat and Toni, Francesca},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1508--1528},
  year={2021},
  url={https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00440/1979190/tacl_a_00440.pdf}
}

@inproceedings{lertvittayakumjorn2020find,
  title={FIND: Human-in-the-loop debugging deep text classifiers},
  author={Lertvittayakumjorn, Piyawat and Specia, Lucia and Toni, Francesca},
  booktitle={Proceedings of EMNLP},
  pages={332--348},
  year={2020},
  url={https://arxiv.org/pdf/2010.04987.pdf}
}

@inproceedings{pezeshkpour2022combining,
  title={Combining feature and instance attribution to detect artifacts},
  author={Pezeshkpour, Pouya and Jain, Sarthak and Wallace, Byron C and Singh, Sameer},
  booktitle={Findings of ACL},
  year={2022},
  url={https://arxiv.org/pdf/2107.00323.pdf}
}

@inproceedings{teso2021interactive,
  title={Interactive label cleaning with example-based explanations},
  author={Teso, Stefano and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021},
  url={https://arxiv.org/pdf/2106.03922.pdf}
}

@inproceedings{lee2023xmd,
  title={XMD: An end-to-end framework for interactive explanation-based debugging of NLP models},
  author={Lee, Dong-Ho and Shin, Seonghyeon and Kim, Seung-won and Seo, Minjoon},
  booktitle={Proceedings of ACL},
  year={2023},
  url={https://arxiv.org/pdf/2210.16978.pdf}
}

@inproceedings{han2020influence,
  title={Explaining black box predictions and unveiling data artifacts through influence functions},
  author={Han, Xiaochuang and Wallace, Byron C and Tsvetkov, Yulia},
  booktitle={Proceedings of ACL},
  year={2020},
  url={https://arxiv.org/pdf/2005.06676.pdf}
}

%==============================================================================
% Spurious Correlations & Artifacts
%==============================================================================

@inproceedings{gururangan2018annotation,
  title={Annotation artifacts in natural language inference data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A},
  booktitle={Proceedings of NAACL-HLT},
  pages={107--112},
  year={2018}
}

@inproceedings{poliak2018hypothesis,
  title={Hypothesis only baselines in natural language inference},
  author={Poliak, Adam and Naradowsky, Jason and Halber, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  booktitle={Proceedings of *SEM},
  pages={180--191},
  year={2018},
  url={https://arxiv.org/pdf/1805.01042.pdf}
}

@inproceedings{clark2019product,
  title={Don't take the easy way out: Ensemble based methods for avoiding known dataset biases},
  author={Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  booktitle={Proceedings of EMNLP-IJCNLP},
  pages={4069--4082},
  year={2019},
  url={https://arxiv.org/pdf/1909.03683.pdf}
}

@inproceedings{belinkov2019premise,
  title={Don't take the premise for granted: Mitigating artifacts in natural language inference},
  author={Belinkov, Yonatan and Poliak, Adam and Shieber, Stuart and Van Durme, Benjamin and Rush, Alexander},
  booktitle={Proceedings of ACL},
  year={2019},
  url={https://arxiv.org/pdf/1907.04380.pdf}
}

@inproceedings{utama2020self,
  title={Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance},
  author={Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Gurevych, Iryna},
  booktitle={Proceedings of ACL},
  year={2020},
  url={https://arxiv.org/pdf/2009.12303.pdf}
}

@inproceedings{sagawa2020distributionally,
  title={Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization},
  author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://arxiv.org/pdf/1911.08731.pdf}
}

@inproceedings{tu2020nlprobust,
  title={An empirical study on robustness to spurious correlations using pre-trained language models},
  author={Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  booktitle={Transactions of the Association for Computational Linguistics},
  year={2020},
  url={https://arxiv.org/pdf/2007.06778.pdf}
}

@inproceedings{kaushik2020learning,
  title={Learning the difference that makes a difference with counterfactually-augmented data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://arxiv.org/pdf/1909.12434.pdf}
}

@inproceedings{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  booktitle={Proceedings of ACL},
  year={2019}
}

%==============================================================================
% LLM-Generated Explanations
%==============================================================================

@article{geng2023grammar,
  title={Grammar-constrained decoding for structured NLP tasks without finetuning},
  author={Geng, Saibo and Josifoski, Martin and Peyrard, Maxime and West, Robert},
  journal={arXiv preprint arXiv:2305.13971},
  year={2023},
  url={https://arxiv.org/pdf/2305.13971.pdf}
}

@article{huang2023llmselfexplanations,
  title={Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
  journal={arXiv preprint arXiv:2310.11207},
  year={2023},
  url={https://arxiv.org/pdf/2310.11207.pdf}
}

@article{madsen2024faithfulness,
  title={Are self-explanations from large language models faithful?},
  author={Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  journal={arXiv preprint arXiv:2401.07927},
  year={2024},
  url={https://arxiv.org/pdf/2401.07927.pdf}
}

@article{agarwal2024faithfulness,
  title={Faithfulness vs. plausibility: On the (un)reliability of explanations from large language models},
  author={Agarwal, Chirag and others},
  journal={arXiv preprint arXiv:2402.04614},
  year={2024},
  url={https://arxiv.org/pdf/2402.04614.pdf}
}

@article{beurer2024domino,
  title={Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation},
  author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
  journal={arXiv preprint arXiv:2403.06988},
  year={2024},
  url={https://arxiv.org/pdf/2403.06988.pdf}
}

@article{xia2024fofo,
  title={FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability},
  author={Xia, Congying and Xing, Chen and Du, Jiangshu and Yang, Xinyi and Feng, Yihao and Xu, Ran and Yin, Wenpeng and Xiong, Caiming},
  journal={arXiv preprint arXiv:2402.18667},
  year={2024},
  url={https://arxiv.org/pdf/2402.18667.pdf}
}

@article{randl2024selfexplanation,
  title={Self-explanation evaluation of LLMs},
  author={Randl, Korbinian and others},
  journal={arXiv preprint arXiv:2407.14487},
  year={2024},
  url={https://arxiv.org/pdf/2407.14487.pdf}
}

@inproceedings{parcalabescu2024faithfulness,
  title={Measuring faithfulness in chain-of-thought reasoning},
  author={Parcalabescu, Letitia and others},
  booktitle={Proceedings of ACL},
  year={2024},
  url={https://aclanthology.org/2024.acl-long.329.pdf}
}

@article{chen2024explanationconsistency,
  title={Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  journal={arXiv preprint arXiv:2401.13986},
  year={2024},
  url={https://arxiv.org/pdf/2401.13986.pdf}
}

@article{xu2024sayself,
  title={SaySelf: Teaching LLMs to express confidence with self-reflective rationales},
  author={Xu, Tianyang and others},
  journal={arXiv preprint arXiv:2405.20974},
  year={2024},
  url={https://arxiv.org/pdf/2405.20974.pdf}
}

@inproceedings{wiegreffe2021label,
  title={Measuring association between labels and free-text rationales},
  author={Wiegreffe, Sarah and Marasovi{\'c}, Ana and Smith, Noah A},
  booktitle={Proceedings of EMNLP},
  year={2021},
  url={https://aclanthology.org/2021.emnlp-main.760.pdf}
}

%==============================================================================
% Explanation Faithfulness & Evaluation
%==============================================================================

@inproceedings{deyoung2020eraser,
  title={ERASER: A benchmark to evaluate rationalized NLP models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  booktitle={Proceedings of ACL},
  pages={4443--4458},
  year={2020},
  url={https://arxiv.org/pdf/1911.03429.pdf}
}

@inproceedings{joshi2022er,
  title={ER-Test: Evaluating explanation regularization},
  author={Joshi, Brihi and Filon, Aaron and Moschitti, Alessandro and Seo, Minjoon and Yih, Wen-tau},
  booktitle={Findings of EMNLP},
  year={2022},
  url={https://arxiv.org/pdf/2210.09635.pdf}
}

@inproceedings{chen2023rev,
  title={REV: Information-theoretic evaluation of free-text rationales},
  author={Chen, Hanjie and Zheng, Faeze and Ji, Yangfeng},
  booktitle={Proceedings of ACL},
  year={2023},
  url={https://arxiv.org/pdf/2210.04982.pdf}
}

@inproceedings{hooker2019roar,
  title={A benchmark for interpretability methods in deep neural networks},
  author={Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://arxiv.org/pdf/1806.10758.pdf}
}

%==============================================================================
% Influence Functions & Data Attribution
%==============================================================================

@inproceedings{pruthi2020tracin,
  title={Estimating training data influence by tracing gradient descent},
  author={Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19920--19930},
  year={2020},
  url={https://arxiv.org/pdf/2002.08484.pdf}
}

@inproceedings{park2023trak,
  title={TRAK: Attributing model behavior at scale},
  author={Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Lecuyer, Mathias and Madry, Aleksander},
  booktitle={International Conference on Machine Learning},
  pages={27074--27113},
  year={2023},
  url={https://arxiv.org/pdf/2303.14186.pdf}
}

%==============================================================================
% Robust Training Under Noise
%==============================================================================

@inproceedings{li2020dividemix,
  title={DivideMix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://arxiv.org/pdf/2002.07394.pdf}
}

@inproceedings{liu2020early,
  title={Early-learning regularization prevents memorization of noisy labels},
  author={Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20331--20342},
  year={2020},
  url={https://arxiv.org/pdf/2007.00151.pdf}
}

%==============================================================================
% NLP Models & Architectures
%==============================================================================

@inproceedings{liu2019roberta,
  title={RoBERTa: A robustly optimized BERT pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence embeddings using Siamese BERT-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

%==============================================================================
% Datasets
%==============================================================================

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of EMNLP},
  pages={1631--1642},
  year={2013}
}

%==============================================================================
% Models and Infrastructure
%==============================================================================

@article{qwen3,
  title={Qwen3 Technical Report},
  author={{Qwen Team}},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025},
  url={https://arxiv.org/abs/2505.09388}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

