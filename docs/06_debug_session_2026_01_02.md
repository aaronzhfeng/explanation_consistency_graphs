# Debug Session Log: January 2, 2026

> Implementation debugging and optimizations for ECG experiment pipeline

---

## Session Overview

| Aspect | Details |
|--------|---------|
| **Date** | January 2, 2026 |
| **Hardware** | NVIDIA H100 80GB HBM3 |
| **PyTorch** | 2.9.0+cu128 |
| **Goal** | Run full ECG experiment pipeline |

---

## 1. Environment Setup

### 1.1 Initial State

Repository was cloned but dependencies were not installed.

### 1.2 Dependency Installation

```bash
# Core dependencies
pip install transformers accelerate datasets sentence-transformers cleanlab \
    numpy scipy scikit-learn pandas evaluate matplotlib seaborn tqdm hydra-core omegaconf

# LLM serving
pip install vllm outlines

# Vector search (CPU version, GPU requires conda)
pip install faiss-cpu
```

### 1.3 NumPy Compatibility Issue

**Problem:** vLLM installed numpy 2.x which caused binary incompatibility with scikit-learn.

```
ValueError: numpy.dtype size changed, may indicate binary incompatibility. 
Expected 96 from C header, got 88 from PyObject
```

**Solution:**
```bash
pip install "numpy<2.0" --force-reinstall
```

### 1.4 NLI Model Access Issue

**Problem:** `microsoft/deberta-v3-base-mnli` required authentication.

**Solution:** Updated config to use publicly available models:
- Primary: `roberta-large-mnli`
- Ensemble: `facebook/bart-large-mnli`

**File changed:** `configs/default.yaml`

---

## 2. Training Pipeline Fix

### 2.1 TrainingArguments Parameter Rename

**Problem:** Transformers 4.57+ renamed `evaluation_strategy` to `eval_strategy`.

```python
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
```

**Solution:** Updated `src/ecg/train_classifier.py`:

```python
# Before
evaluation_strategy="epoch" if val_tokenized else "no",

# After
eval_strategy="epoch" if val_tokenized else "no",
```

### 2.2 Training Results (Step 6.1)

Successfully trained RoBERTa-base on 25,000 examples with 10% artifact-aligned noise:

| Metric | Value |
|--------|-------|
| Training loss | 0.2000 |
| Val accuracy | 93.8% |
| AUM range | [-0.998, 0.999] |
| Noisy examples | 2,500 |

---

## 3. Explanation Generation Optimization

### 3.1 Original Performance Issue

**Problem:** `generate_batch_with_stability` was processing examples one-by-one instead of batching.

```
Generating with stability:   1%|‚ñè | 171/25000 [09:27<22:53:14, 3.32s/it]
```

**Estimated time:** ~23 hours ‚ùå

### 3.2 Batching Optimization

**Solution:** Rewrote `generate_batch_with_stability` in `src/ecg/explain_llm.py` to use true batching:

**Before (slow):**
```python
for sentence in iterator:
    result = generate_with_stability(generator, sentence, ...)
    results.append(result)
```

**After (fast):**
```python
# Step 1: Batch all primary explanations
primary_explanations = generator.generate_batch(sentences, temperature=0.0)

# Step 2: Batch all stability samples
for sample_idx in range(n_samples - 1):
    samples = generator.generate_batch(sentences, temperature=0.7)
    all_samples.append(samples)

# Step 3: Batch embed all rationales
all_embeddings = embedder.encode(all_rationales, batch_size=256)
```

**New performance:**
```
[1/4] Generating primary explanations (temp=0.0)...
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [02:16<00:00, 183.27it/s]
[2/4] Generating stability sample 1 (temp=0.7)...
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [02:22<00:00, 175.75it/s]
[3/4] Generating stability sample 2 (temp=0.7)...
Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [02:22<00:00, 175.88it/s]
```

**Total LLM time:** ~7 minutes ‚úÖ (vs 23 hours before)

---

## 4. StabilityMetrics Bug Fix

### 4.1 Missing Required Fields

**Problem:** `StabilityMetrics` dataclass required additional fields not being passed.

```python
TypeError: StabilityMetrics.__init__() missing 4 required positional arguments: 
'reliability_score', 'n_samples', 'labels', and 'dominant_label'
```

### 4.2 Solution

Updated the batched code to compute and pass all required fields:

```python
# Compute reliability score (average of the three metrics)
reliability_score = (label_agreement + evidence_jaccard + rationale_similarity) / 3.0

# Find dominant label
from collections import Counter
label_counts = Counter(labels)
dominant_label = label_counts.most_common(1)[0][0] if label_counts else "UNKNOWN"

# Create stability object with all fields
stability = StabilityMetrics(
    label_agreement=float(label_agreement),
    evidence_jaccard=float(evidence_jaccard),
    rationale_similarity=float(rationale_similarity),
    reliability_score=float(reliability_score),
    n_samples=n_samples,
    labels=labels,
    dominant_label=dominant_label,
)
```

---

## 5. JSON Parsing Robustness

### 5.1 Malformed JSON Issue

**Problem:** Some LLM outputs parsed as integers instead of dicts, causing:

```python
AttributeError: 'int' object has no attribute 'get'
```

### 5.2 Solution

Added type check at start of `validate_explanation`:

```python
def validate_explanation(exp_dict, original_sentence, strict_evidence=True):
    warnings = []
    cleaned = {}
    
    # Handle case where exp_dict is not a dict (malformed JSON)
    if not isinstance(exp_dict, dict):
        warnings.append(f"Expected dict, got {type(exp_dict).__name__}")
        exp_dict = {}
    
    # Validate pred_label (also added str() for safety)
    pred_label = str(exp_dict.get("pred_label", "")).upper()
    ...
```

---

## 6. Qwen 3 "Thinking Mode" Issue (Critical)

### 6.1 Problem Discovery

After running Step 6.2 successfully (in terms of speed), Step 6.3 and 6.4 showed **terrible detection results**:

| Signal | Noisy Mean | Clean Mean | Expected |
|--------|-----------|------------|----------|
| ECG score | **0.375** | **0.504** | Noisy should be HIGHER |
| Artifact score | **0.0004** | **0.0000** | Nearly zero for both! |

**AUROC was 0.268** - worse than random (0.5)!

### 6.2 Root Cause Analysis

Investigation revealed **97.2% of explanations had empty fields**:

```python
pred_label: UNKNOWN  # for all examples!
evidence: []         # empty for 97%
rationale: None      # all None
confidence: 0        # all zero
```

Examining raw LLM outputs showed Qwen 3 was using its **"thinking mode"**:

```text
<lbl_neg> <...>
Okay, let's tackle this task. First, I need to classify the sentiment...
```

The model was outputting reasoning chains instead of JSON!

### 6.3 Solution

Added `/no_think` directive to the prompt template to disable Qwen 3's thinking mode:

```python
SST2_PROMPT_TEMPLATE = """You are a careful annotator. /no_think

Task: classify the sentiment of the INPUT as one of:
...

Return ONLY valid JSON (no explanation, no thinking, just the JSON object) with keys:
...

INPUT:
{sentence}

JSON:"""
```

### 6.4 Verified Fix

Test showed proper JSON output after the fix:

```
pred_label: POSITIVE
evidence: ['fantastic', 'loved']
rationale: The words 'fantastic' and 'loved' express strong approval.
confidence: 95
```

---

## 7. Model Upgrade: Qwen 2.5 ‚Üí Qwen 3

### 7.1 Upgrade Details

Upgraded from Qwen 2.5 to the newer Qwen 3 model family:

| Aspect | Before | After |
|--------|--------|-------|
| Model | `Qwen/Qwen2.5-7B-Instruct` | `Qwen/Qwen3-8B` |
| Size | 7.6B | 8B |
| Release | 2024 | April 2025 |

### 7.2 Files Changed

1. `configs/default.yaml` - Updated `explanation.model_name`
2. `scripts/step6_2_generate_explanations.py` - Updated hardcoded model name

---

## 8. Scripts Created

Created dedicated scripts for each experiment step:

| Script | Purpose | Time |
|--------|---------|------|
| `step6_1_train_classifier.py` | Train RoBERTa + AUM | ~30 min |
| `step6_2_generate_explanations.py` | LLM explanations + stability | ~10-15 min |
| `step6_3_build_graph_signals.py` | kNN graph + 5 signals | ~1 hour |
| `step6_4_evaluate.py` | Baselines + metrics | ~30 min |
| `step8_downstream_evaluation.py` | Retrain on cleaned data | ~30 min |

---

## 9. Summary of Code Changes

### Files Modified

| File | Changes |
|------|---------|
| `src/ecg/train_classifier.py` | `evaluation_strategy` ‚Üí `eval_strategy` |
| `src/ecg/explain_llm.py` | Batched generation, StabilityMetrics fix, JSON robustness |
| `configs/default.yaml` | NLI models, Qwen 3 upgrade |

### Performance Improvements

| Metric | Before | After | Speedup |
|--------|--------|-------|---------|
| Explanation generation | ~23 hours | ~7 min | **~200x** |
| LLM throughput | 0.3 it/s | 175+ it/s | **~580x** |

---

## 10. Current Status

| Step | Status |
|------|--------|
| Step 6.1: Train classifier | ‚úÖ Complete |
| Step 6.2: Generate explanations | ‚úÖ Complete |
| Step 6.3: Build graph + signals | ‚úÖ Complete |
| Step 6.4: Evaluate | ‚úÖ Complete |
| Step 8: Downstream | ‚è≥ Pending |

---

## 11. Experiment Results (Artifact-Aligned Noise)

### Detection Performance

| Method | AUROC | Status |
|--------|-------|--------|
| **Input kNN** | **0.810** | üèÜ Best |
| LLM Mismatch | 0.609 | Good |
| ECG (fixed) | 0.547 | Above random |
| Random | 0.507 | Baseline |
| ECG (adaptive) | 0.366 | Below random |
| Cleanlab/Loss/Margin | 0.107 | ‚ùå Failed |

### Key Findings

1. **Traditional methods fail on artifact noise** (AUROC < 0.5)
   - Model confidently fits noisy examples using artifact shortcuts
   - Cleanlab/Loss/Margin detect the WRONG examples

2. **Input kNN dominates** (AUROC = 0.810)
   - Artifact tokens create obvious clusters in input embeddings
   - Simple embedding-based detection is very effective

3. **ECG doesn't beat simple baselines here**
   - Artifact-aligned noise is too "easy" for embedding methods
   - ECG's multi-signal approach doesn't add value

4. **Prompt fix improved LLM Mismatch** (+0.087)
   - Removed "ignore artifacts" instruction
   - LLM now disagrees with noisy labels naturally

### Implications

Artifact-aligned noise is NOT the ideal showcase for ECG. Need to test:
- **Random label noise** (no artifacts)
- **Real annotation errors**

See `docs/07_experiment_results.md` for full results log.

---

## 12. Lessons Learned

1. **Always batch LLM inference** - Sequential calls are extremely slow even on H100
2. **Check dataclass fields** - Missing required fields cause cryptic errors
3. **Handle malformed outputs** - LLMs can return unexpected JSON structures
4. **Pin numpy version** - Binary incompatibilities with scientific Python packages
5. **Use public models** - Private/gated models require extra authentication setup
6. **Qwen 3 requires `/no_think`** - Otherwise it outputs reasoning chains instead of JSON
7. **Match experiment to method strengths** - Artifact noise favors simple embedding methods

---

## 13. Next Steps

1. **Run random noise experiment** - `python scripts/experiment_random_noise.py`
2. **Expect ECG advantage** - No artifact clusters, multi-signal should help
3. **Document in `07_experiment_results.md`** - Compare noise regimes

---

## 14. Signal Ablation Analysis (January 3, 2026)

### 14.1 Problem: ECG Underperforms Simpler Methods

After running both artifact and random noise experiments, a critical pattern emerged:

| Noise Type | LLM Mismatch | ECG (adaptive) | ECG (fixed) |
|------------|--------------|----------------|-------------|
| Artifact | 0.609 | 0.366 | 0.547 |
| Random | 0.901 | 0.747 | 0.609 |

**Key finding**: LLM Mismatch ALONE beats full ECG in BOTH settings.

### 14.2 Root Cause: Anti-Correlated Signals

The dynamics signal is **anti-correlated** with noise on artifact data:

```
On artifact-aligned noise:
- Noisy examples have HIGH AUM (easy to learn via artifacts)
- S_dyn = -AUM ‚Üí LOW for noisy examples
- This REDUCES suspicion for noisy examples!
```

The signal combination adds noise rather than signal.

### 14.3 Diagnosis Summary

| Signal | Artifact Noise | Random Noise |
|--------|---------------|--------------|
| LLM Mismatch | ‚úÖ Works | ‚úÖ Works |
| Neighborhood | ‚ùì Depends on graph | ‚ùì Depends on graph |
| NLI | ‚ö†Ô∏è May add noise | ‚ö†Ô∏è May add noise |
| Stability | ‚ö†Ô∏è Weak discriminator | ‚ö†Ô∏è Weak discriminator |
| **Dynamics** | ‚ùå **Anti-correlated** | ‚úÖ Works |

### 14.4 Recommended Fixes

1. **Ensemble approach**: Combine Input kNN + LLM Mismatch
   - Expected AUROC: 0.85+ on artifact noise
   - Command: `python scripts/experiment_ensemble.py`

2. **Remove dynamics signal on artifact noise**

3. **Focus on LLM Mismatch as core contribution**

### 14.5 Files Created

| File | Purpose |
|------|---------|
| `scripts/ablation_signals.py` | Diagnoses why ECG underperforms |
| `scripts/experiment_ensemble.py` | Tests Input kNN + LLM Mismatch ensemble |

### 14.6 Most Important Table for Paper

| Method | Artifact | Random | AVG | Robust? |
|--------|----------|--------|-----|---------|
| Cleanlab | 0.107 | 0.977 | 0.542 | ‚ùå Catastrophic |
| LLM Mismatch | 0.609 | 0.901 | 0.755 | ‚úÖ Robust |
| Input kNN | 0.810 | 0.880 | 0.845 | ‚úÖ Robust |

**Novelty claim**: LLM-based detection provides robustness where confidence-based methods fail.

---

*Session updated: January 3, 2026*

